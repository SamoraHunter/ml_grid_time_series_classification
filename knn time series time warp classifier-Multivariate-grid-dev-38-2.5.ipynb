{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Set to the desired GPU index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bad080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_gpu():\n",
    "    gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "    gpu_df = pd.read_csv(StringIO(gpu_stats.decode('utf-8')),\n",
    "                         names=['memory.used', 'memory.free'],\n",
    "                         skiprows=1)\n",
    "    #print('GPU usage:\\n{}'.format(gpu_df))\n",
    "    gpu_df['memory.free'] = gpu_df['memory.free'].map(lambda x: x.rstrip(' [MiB]'))\n",
    "    idx = gpu_df['memory.free'].astype(int).idxmax()\n",
    "    print('Returning GPU{} with {} free MiB'.format(idx, gpu_df.iloc[idx]['memory.free']))\n",
    "    return int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_free_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886394",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade aeon[all_extras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --upgrade dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pycatch22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250363b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "import traceback\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8177f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install imblearn-0.0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24399178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c46e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall tensorflow-probability -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca5af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow-probability==0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2feaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow-probability==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install aeon[all_extras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using cached tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
    "#Using cached tensorflow_probability-0.20.1-py2.py3-none-any.whl (6.9 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae2ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall aeon[all_extras] -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ba099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.dictionary_based import IndividualTDE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import aeon\n",
    "from aeon.classification.deep_learning.encoder import EncoderClassifier #req tfa\n",
    "from aeon.classification.deep_learning.resnet import ResNetClassifier\n",
    "from aeon.classification.ordinal_classification import OrdinalTDE\n",
    "from aeon.classification.deep_learning.inception_time import InceptionTimeClassifier\n",
    "from aeon.classification.feature_based._fresh_prince import FreshPRINCEClassifier #cuda 11\n",
    "import keras\n",
    "from aeon.classification.dictionary_based import ContractableBOSS\n",
    "from aeon.classification.dictionary_based._muse import MUSE\n",
    "from aeon.classification.feature_based._tsfresh_classifier import TSFreshClassifier #cuda\n",
    "from aeon.classification.deep_learning.inception_time import IndividualInceptionClassifier\n",
    "from aeon.classification.feature_based._signature_classifier import SignatureClassifier\n",
    "from aeon.classification.feature_based._summary_classifier import SummaryClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pycatch22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e11f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fuzzysearch import find_near_matches\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.convolution_based import RocketClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.deep_learning import *\n",
    "from aeon.classification.distance_based  import *\n",
    "from aeon.classification.convolution_based  import *\n",
    "from aeon.classification.convolution_based  import *\n",
    "from aeon.classification.hybrid  import *\n",
    "from aeon.classification.interval_based import *\n",
    "\n",
    "#from aeon.classification.feature_based import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.interval_based import DrCIFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.feature_based import FreshPRINCEClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.feature_based import Catch22Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c3ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f80be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "X = [[[1, 2, 3, 4, 5, 6, 7]],  # 3D array example (univariate)\n",
    "     [[4, 4, 4, 5, 6, 7, 3]]]  # Two samples, one channel, seven series length\n",
    "y = [0, 1]  # class labels for each sample\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "clf.fit(X, y)  # fit the classifier on train data\n",
    "\n",
    "X_test = np.array([[2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4]])\n",
    "y_pred = clf.predict(X_test)  # make class predictions on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927964ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "     [[1, 2, 3, 4, 5, 6, 7]],  # 3D array example (univariate)\n",
    "     [[4, 4, 4, 5, 6, 7, 3]],\n",
    "     [[4, 4, 4, 3, 6, 7, 3]]\n",
    "    ]  # Two samples, one channel, seven series length\n",
    "y = [0, 1, 1]  # class labels for each sample\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "clf.fit(X, y)  # fit the classifier on train data\n",
    "\n",
    "X_test = np.array([[2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4]])\n",
    "y_pred = clf.predict(X_test)  # make class predictions on new data\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458984a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].sum(), X[1].sum(), X[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b128512",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multivariate case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0565764",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\n",
    "     [[1, 2, 3, 4, 5, 6, 7], [1, 2, 3, 4, 5, 6, 7]],  # 3D array example (univariate)\n",
    "     [[4, 4, 4, 5, 6, 7, 3], [4, 4, 4, 5, 6, 7, 3]]\n",
    "    ]  # Two samples, one channel, seven series length\n",
    "y = [0, 1]  # class labels for each sample\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "clf.fit(X, y)  # fit the classifier on train data\n",
    "\n",
    "X_test = np.array([[2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4]])\n",
    "y_pred = clf.predict(X_test)  # make class predictions on new data\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ebadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdde9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c258c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empty_date_vector():\n",
    "    months = [x for x in range(1,13)]\n",
    "    years = [x for x in range(1995, 2023)]\n",
    "    combinations = list(itertools.product(years, months))\n",
    "    combinations = [str(item) + '_' + 'date_time_stamp' for item in combinations]\n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(data=0.0, index=np.arange(1), columns = combinations).astype(float) #untested float cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aae5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(col):\n",
    "    \n",
    "    \n",
    "    return ordered_date_vector_columns.index(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_row_count_data  = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv',engine=\"pyarrow\", usecols=['age']) #/data/AS/Samora/HFE/HFE/v21/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_row_count_data  = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', usecols=['age']) #/data/AS/Samora/HFE/HFE/v21/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'hfe_TC_merge_T_Im_10k_1yr.csv'\n",
    "\n",
    "with open(csv_file_path, 'r') as file:\n",
    "    n_row_count_data = sum(1 for _ in file)\n",
    "\n",
    "print(\"Number of rows:\", n_row_count_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d51fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf114e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row_count = n_row_count_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efc544",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Sample\n",
    "n = n_row_count #number of records in file\n",
    "s = 1 #desired sample size\n",
    "\n",
    "filename = 'hfe_TC_merge_T_Im_10k_1yr.csv' #/data/AS/Samora/HFE/HFE/v21/\n",
    "\n",
    "skip = sorted(random.sample(range(n),n-s))[1:]\n",
    "\n",
    "#dfo = pd.read_csv(filename, skiprows=skip, header=0)\n",
    "#dfo = pd.read_csv(filename, skiprows=skip, index_col=False)\n",
    "#df = pd.read_csv(filename, engine=\"pyarrow\", skiprows=skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(dfo['client_idcode'].sample(30).unique()).to_csv('client_idcode_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ecc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#All data\n",
    "#dfo = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv',engine=\"pyarrow\") # /data/AS/Samora/HFE/HFE/v21/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98558337",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2361441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_cols = dc.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98610d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {element: float for element in float_cols}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62945052",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict['client_idcode'] = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa113ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = pd.read_csv('missing_combined_10kdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee41f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df[missing_df['percent_missing']<99.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df['percent_missing'].plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd2803",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df[missing_df['percent_missing']>90].plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_col_selection = missing_df[missing_df['percent_missing']<99.999999999]['column_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae08969",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_col_selection = missing_df[missing_df['percent_missing']>0.000000000001]['column_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(read_col_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557684fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9701fa90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#All data\n",
    "#dfo = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', dtype=result_dict) # /data/AS/Samora/HFE/HFE/v21/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b666a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_polars = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = False\n",
    "sub_sample_columns = False # select 50 first cols\n",
    "\n",
    "col_sample_n = 25\n",
    "sample_n = 50\n",
    "\n",
    "sub_sample_columns_init = False\n",
    "\n",
    "sub_sample_feature_importances_columns = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_date_vector_columns = list(get_empty_date_vector().columns)\n",
    "if(sub_sample_columns and sub_sample_columns_init):\n",
    "    read_col_selection_set = set(pd.Series(read_col_selection).sample(col_sample_n))\n",
    "    read_col_selection_set.add('client_idcode')\n",
    "    read_col_selection_set.add('Ferritin_mean')\n",
    "    read_col_selection_set.add('outcome_var_1')\n",
    "    read_col_selection_set.add('male')\n",
    "    read_col_selection_set.add('age')\n",
    "    \n",
    "    a = list(read_col_selection_set)\n",
    "    a.extend(ordered_date_vector_columns)\n",
    "    read_col_selection = list(set(a))\n",
    "    len(read_col_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sample_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008818ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(sub_sample_feature_importances_columns):\n",
    "    fi = pd.read_csv('importance_df2.csv')\n",
    "    \n",
    "    read_col_selection_set = set(pd.Series(fi['Feature'].to_list()).sample(col_sample_n))\n",
    "    read_col_selection_set.add('client_idcode')\n",
    "    read_col_selection_set.add('Ferritin_mean')\n",
    "    read_col_selection_set.add('outcome_var_1')\n",
    "    read_col_selection_set.add('male')\n",
    "    read_col_selection_set.add('age')\n",
    "    \n",
    "    a = list(read_col_selection_set)\n",
    "    a.extend(ordered_date_vector_columns)\n",
    "    read_col_selection = list(set(a))\n",
    "    print(len(read_col_selection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12afd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfp = '/mnt/hdd1/samora/hfe_v23/hfe_TC_merge_T_Im_10k_1yr_forward_backward_imp_M_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f72606",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if(use_polars == False):\n",
    "    #dfo = pd.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', dtype={'client_idcode':str}, usecols=read_col_selection)\n",
    "    \n",
    "    dfo = pd.read_csv(nfp, dtype={'client_idcode':str}, usecols=read_col_selection)\n",
    "else:\n",
    "    \n",
    "    import polars as pl\n",
    "    \n",
    "    schema = [\n",
    "    (\"client_idcode\", pl.Utf8),  # Use Utf8 type for the non-float64 column\n",
    "    # Add more columns as needed\n",
    "    ]\n",
    "\n",
    "    target_col_list = dc.columns\n",
    "\n",
    "    for i in range(0, len(target_col_list)):\n",
    "        schema.append((target_col_list[i], pl.Float64))\n",
    "\n",
    "    dfp = pl.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', ignore_errors =True) #schema=schema\n",
    "\n",
    "    #dfp = pl.read_csv('hfe_TC_merge_T_Im_10k_1yr.csv', schema=schema)\n",
    "    \n",
    "    pandas_df = dfp.to_pandas()\n",
    "    \n",
    "    dfo = pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_polars == True):\n",
    "    pl.DataFrame.estimated_size(dfp)\n",
    "    \n",
    "    dfp = None\n",
    "    \n",
    "    del dfp\n",
    "    del pl\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07362a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8210dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_idcode_sample_list = dfo['client_idcode'].sample(sample_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fe0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo[dfo['client_idcode'].isin(client_idcode_sample_list)]['outcome_var_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(sample_data):\n",
    "    dfo_sample = dfo[dfo['client_idcode'].isin(client_idcode_sample_list)].copy()\n",
    "else:\n",
    "    dfo_sample = dfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "20*336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92ca893",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_sample['client_idcode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#df = dfo[0:3000].copy()\n",
    "\n",
    "df = dfo_sample#.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c389b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93113e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get a list of column names except \"client_idcode\" that are of type 'object'\n",
    "columns_to_cast = [col for col in df.columns if col != \"client_idcode\" and df[col].dtype == 'object']\n",
    "\n",
    "# Cast selected columns to float64\n",
    "df[columns_to_cast] = df[columns_to_cast].astype('float64')\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# columns_to_cast = [col for col in df.columns if col != \"client_idcode\"]\n",
    "\n",
    "# # Cast selected columns to float64\n",
    "# df[columns_to_cast] = df[columns_to_cast].astype('float64')\n",
    "\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['outcome_var_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d67dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fi\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# # Assuming your DataFrame is named 'time_series_df'\n",
    "# # Ensure that your DataFrame is appropriately formatted with datetime index and feature columns\n",
    "\n",
    "# # Split the data into features (X) and target variable (y)\n",
    "# X = df.drop('outcome_var_1', axis=1)  # Adjust 'target_column' to your target column's name\n",
    "# y = time_series_df['outcome_var_1']\n",
    "\n",
    "# # Split into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# # Initialize and train a Random Forest regressor\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importances\n",
    "# feature_importances = model.feature_importances_\n",
    "\n",
    "# # Create a DataFrame to display the feature importances\n",
    "# importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
    "# importance_df = importance_df.sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc13235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d4a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e914ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl = list(df.columns)\n",
    "\n",
    "acl\n",
    "\n",
    "dcl = [x for x in acl if \"_date_time_stamp\" in x]\n",
    "\n",
    "dcl.sort()\n",
    "\n",
    "dcl[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def get_date(row):\n",
    "#     for col in dcl:\n",
    "#         if row[col] == 1:\n",
    "#             return col.split(\"_\")[0]\n",
    "#     return None\n",
    "\n",
    "# # apply the custom function to create a new column with the date string\n",
    "# df['date'] = df.apply(get_date, axis=1) \n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcl_data_slice = df[dcl].copy()\n",
    "\n",
    "def get_date(row):\n",
    "    for col in dcl:\n",
    "        if row[col] == 1:\n",
    "            return col.split(\"_\")[0]\n",
    "    return None\n",
    "\n",
    "date_list_data = []\n",
    "\n",
    "for i in tqdm(range(0, len(df))):\n",
    "    date_list_data.append(get_date(dcl_data_slice.iloc[i]))\n",
    "\n",
    "df['date'] = date_list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_date(row):\n",
    "#     for col in df.columns:\n",
    "#         if row[col] == 1:\n",
    "#             return col\n",
    "#     return None\n",
    "\n",
    "# # apply the custom function to create a new column with the date string\n",
    "# df['date'] = df.apply(get_date, axis=1).apply(lambda x: f\"{x}\".split(\"_\")[0] if x is not None else None)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d071c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_date(row):\n",
    "#     for col, value in row.iteritems():\n",
    "#         if value == 1:\n",
    "#             return col\n",
    "#     return None\n",
    "\n",
    "# # Apply the custom function to create a new column with the column name\n",
    "# df['date'] = df.apply(get_date, axis=1)\n",
    "\n",
    "# # Extract the date string from the column name\n",
    "# df['date'] = df['date'].str.split('_').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76874fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_date_vector_columns = list(get_empty_date_vector().columns)\n",
    "ordered_date_vector_columns = [x.split(\"_\")[0] for x in ordered_date_vector_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df.drop(['__index_level_0__'], inplace=True, axis=1)\n",
    "    \n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df.drop([ 'level_0'], inplace=True, axis=1)\n",
    "    \n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['level'], inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55adbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unknown crash\n",
    "# def get_date(row):\n",
    "#     for col in df.columns:\n",
    "#         if row[col] == 1:\n",
    "#             return col\n",
    "#     return None\n",
    "\n",
    "# # apply the custom function to create a new column with the date string\n",
    "# df['date'] = df.apply(get_date, axis=1).apply(lambda x: f\"{x}\".split(\"_\")[0] if x is not None else None)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae45c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#why twice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b3add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcl_data_slice = df[dcl].copy()\n",
    "\n",
    "def get_date(row):\n",
    "    for col in dcl:\n",
    "        if row[col] == 1:\n",
    "            return col.split(\"_\")[0]\n",
    "    return None\n",
    "\n",
    "date_list_data = []\n",
    "\n",
    "for i in tqdm(range(0, len(df))):\n",
    "    date_list_data.append(get_date(dcl_data_slice.iloc[i]))\n",
    "\n",
    "df['date'] = date_list_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date_order_sequence'] = df['date'].apply(get_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['client_idcode', 'date_order_sequence'] + [col for col in df.columns if col not in ['client_idcode', 'date_order_sequence']]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd43abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a03370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e38ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col2 =[col for col in df.columns if col != 'outcome_var_1'] + ['outcome_var_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(cols) - set(col2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a16ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols == col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67186160",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96037ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = [col for col in df.columns if col != 'outcome_var_1'] + ['outcome_var_1']\n",
    "#df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['client_idcode'] = le.fit_transform(df['client_idcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f98b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b5c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crashing red?\n",
    "#df = df.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col = 'Ferritin_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cfa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.columns[350:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b27812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['client_idcode', 'date_order_sequence', test_col,  'outcome_var_1']][test_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9bd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['client_idcode', 'date_order_sequence', test_col,  'outcome_var_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6170be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['client_idcode', 'date_order_sequence', test_col,  'outcome_var_1']][test_col].plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6745bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45c507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef24442",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1846533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfo.select_dtypes(include=numerics).std().sort_values(ascending=False).index[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfo.columns:\n",
    "    if(col.lower().find(\"median\")!=-1):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13768659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop to nest and prepare patient vector sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pats = df[['client_idcode', 'date_order_sequence', test_col,  'outcome_var_1']]['client_idcode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['client_idcode']==8].iloc[0]['outcome_var_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['client_idcode']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 336\n",
    "# sequence.pad_sequences(X_train, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_col_vals_raw = df[df['client_idcode']==8][test_col].to_list()\n",
    "pat_col_vals_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5502f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.pad_sequences([pat_col_vals_raw], maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d937ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute or pad zero? \n",
    "# pad from front or the back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26585606",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNfeaturesANOVAF(n):\n",
    "    res = []\n",
    "    for colName in X_train.columns:\n",
    "        if colName != \"intercept\":\n",
    "            res.append(\n",
    "                (\n",
    "                    colName,\n",
    "                    sklearn.feature_selection.f_classif(\n",
    "                        np.array(X_train[colName]).reshape(-1, 1), y_train\n",
    "                    )[0],\n",
    "                )\n",
    "            )\n",
    "    sortedList = sorted(res, key=lambda x: x[1])\n",
    "    sortedList.reverse()\n",
    "    nFeatures = sortedList[:n]\n",
    "    finalColNames = []\n",
    "    for elem in nFeatures:\n",
    "        finalColNames.append(elem[0])\n",
    "    return finalColNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0106492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNfeaturesANOVAF_spec(n, X_train, y_train):\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(len(X_train.columns))\n",
    "    \n",
    "    res = []\n",
    "    for colName in X_train.columns:\n",
    "        if colName != \"intercept\":\n",
    "            res.append(\n",
    "                (\n",
    "                    colName,\n",
    "                    sklearn.feature_selection.f_classif(\n",
    "                        np.array(X_train[colName]).reshape(-1, 1), y_train\n",
    "                    )[0],\n",
    "                )\n",
    "            )\n",
    "    sortedList = sorted(res, key=lambda x: x[1])\n",
    "    sortedList.reverse()\n",
    "    nFeatures = sortedList[:n]\n",
    "    finalColNames = []\n",
    "    for elem in nFeatures:\n",
    "        finalColNames.append(elem[0])\n",
    "    return finalColNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "grid = {\n",
    "    \n",
    "    'resample' : ['undersample'],\n",
    "    'scale'    : [True],\n",
    "    'n_features': [ 25, 5],\n",
    "    'param_space_size':['medium'],\n",
    "    'n_unique_out': [10],\n",
    "    'outcome_var_n':['1'],\n",
    "    'percent_missing':[99.99, 99.95,],  #n/100 ex 95 for 95%\n",
    "                     'corr':[0.8, 0.5],\n",
    "                     'data':[{'age':[True],\n",
    "                            'sex':[True],\n",
    "                             'bmi':[True],\n",
    "                             'ethnicity':[True],\n",
    "                            'bloods':[True, False],\n",
    "                            'diagnostic_order':[True, False],\n",
    "                            'drug_order':[True, False],\n",
    "                            'annotation_n':[True, False],\n",
    "                            'meta_sp_annotation_n':[True, False],\n",
    "                             'annotation_mrc_n':[True, False],\n",
    "                             'meta_sp_annotation_mrc_n':[True, False],\n",
    "                              'core_02':[False],\n",
    "                            'bed':[False],\n",
    "                            'vte_status':[True],\n",
    "                            'hosp_site':[True],\n",
    "                            'core_resus':[False],\n",
    "                            'news':[False],\n",
    "                            'date_time_stamp':[ False]\n",
    "                             \n",
    "                             }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef4760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "    \n",
    "#     'resample' : ['undersample'],\n",
    "#     'scale'    : [True],\n",
    "#     'feature_n': [ 'all'],\n",
    "#     'param_space_size':['medium'],\n",
    "#     'n_unique_out': [10],\n",
    "#     'outcome_var_n':['1'],\n",
    "#     'percent_missing':[99.99, 99.95,],  #n/100 ex 95 for 95%\n",
    "#                      'corr':[0.8, 0.5],\n",
    "#                      'data':[{'age':[True],\n",
    "#                             'sex':[True],\n",
    "#                              'bmi':[True],\n",
    "#                              'ethnicity':[True],\n",
    "#                             'bloods':[True],\n",
    "#                             'diagnostic_order':[ False],\n",
    "#                             'drug_order':[ False],\n",
    "#                             'annotation_n':[ False],\n",
    "#                             'meta_sp_annotation_n':[ False],\n",
    "#                              'annotation_mrc_n':[ False],\n",
    "#                              'meta_sp_annotation_mrc_n':[ False],\n",
    "#                               'core_02':[False],\n",
    "#                             'bed':[False],\n",
    "#                             'vte_status':[True],\n",
    "#                             'hosp_site':[True],\n",
    "#                             'core_resus':[False],\n",
    "#                             'news':[False],\n",
    "#                             'date_time_stamp':[ False]\n",
    "                             \n",
    "#                              }]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_prod(d):\n",
    "    if isinstance(d, list):\n",
    "         for i in d:\n",
    "            yield from ([i] if not isinstance(i, (dict, list)) else c_prod(i))\n",
    "    else:\n",
    "         for i in it.product(*map(c_prod, d.values())):\n",
    "            yield dict(zip(d.keys(), i))\n",
    "\n",
    "print(len(list(c_prod(grid))))\n",
    "settings_list = list(c_prod(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebd962",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_param = 1\n",
    "time_limit_param = [3] # what does 0 do? #optimal param?\n",
    "random_state_val = 0 # this is for model config not data space\n",
    "\n",
    "n_jobs_model_val = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c1093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_project_dir_global = 'HFE_ML_sequence_experiments/'\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True) \n",
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "base_project_dir = 'HFE_ML_sequence_experiments/' + st_time + \"/\"\n",
    "additional_naming = \"HFE_ML_Grid_\"\n",
    "\n",
    "outcome_var = 'outcome_var'\n",
    "\n",
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "base_project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_list = {'auc': make_scorer(roc_auc_score, needs_proba=False),\n",
    "                'f1':'f1',\n",
    "                'accuracy':'accuracy',\n",
    "                'recall': 'recall'}\n",
    "\n",
    "#init final grid scores\n",
    "column_list = ['algorithm_implementation', 'parameter_sample','method_name', 'nb_size', 'f_list', 'auc','mcc','f1','precision','recall','accuracy',\n",
    "                       \n",
    "                 'resample', 'scale', 'n_features', 'param_space_size', 'n_unique_out',\n",
    "                 'outcome_var_n', 'percent_missing', 'corr', \n",
    "                 'age', 'sex', 'bmi','ethnicity', 'bloods', 'diagnostic_order',\n",
    "                 'drug_order', 'annotation_n', 'meta_sp_annotation_n',\n",
    "                 'meta_sp_annotation_mrc_n','annotation_mrc_n',\n",
    "                 'core_02','bed','vte_status','hosp_site','core_resus','news',\n",
    "                 'X_train_size', 'X_test_orig_size', 'X_test_size', 'run_time', 'n_fits', 't_fits', 'i'\n",
    "                        \n",
    "              ]\n",
    "\n",
    "metric_names = []\n",
    "for metric in metric_list:\n",
    "    metric_names.append(f'{metric}_m')\n",
    "    metric_names.append(f'{metric}_std')\n",
    "\n",
    "column_list.extend(metric_names)\n",
    "\n",
    "#column_list = column_list +['BL_' + str(x) for x in range(0, 64)]\n",
    "\n",
    "df_log = pd.DataFrame( data = None, columns = column_list)\n",
    "\n",
    "df_log.to_csv(base_project_dir + 'final_grid_score_log.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa1886",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outcome_list = ['Hemochromatosis (disorder)_count',\n",
    " 'Hemochromatosis (disorder)_count_subject_present',\n",
    " 'Hemochromatosis (disorder)_count_relative_present',\n",
    " 'Bronze cirrhosis (disorder)_count',\n",
    " 'Bronze cirrhosis (disorder)_count_subject_present',\n",
    " 'Bronze cirrhosis (disorder)_count_relative_present',\n",
    " 'Bronze diabetes (disorder)_count',\n",
    " 'Bronze diabetes (disorder)_count_subject_present',\n",
    " 'Bronze diabetes (disorder)_count_relative_present',\n",
    " 'Hereditary hemochromatosis (disorder)_count',\n",
    " 'Hereditary hemochromatosis (disorder)_count_subject_present',\n",
    " 'Hereditary hemochromatosis (disorder)_count_relative_present',\n",
    " 'Autosomal dominant hereditary hemochromatosis (disorder)_count',\n",
    " 'Autosomal dominant hereditary hemochromatosis (disorder)_count_subject_present',\n",
    " 'Autosomal dominant hereditary hemochromatosis (disorder)_count_relative_present',\n",
    " '1186847009_count',\n",
    " '1186847009_count_subject_present',\n",
    " '1186847009_count_relative_present',\n",
    " 'Hemochromatosis type 3 (disorder)_count',\n",
    " 'Hemochromatosis type 3 (disorder)_count_subject_present',\n",
    " 'Hemochromatosis type 3 (disorder)_count_relative_present',\n",
    " 'Juvenile hemochromatosis (disorder)_count',\n",
    " 'Juvenile hemochromatosis (disorder)_count_subject_present',\n",
    " 'Juvenile hemochromatosis (disorder)_count_relative_present',\n",
    " '1186849007_count',\n",
    " '1186849007_count_subject_present',\n",
    " '1186849007_count_relative_present',\n",
    " '1186844002_count',\n",
    " '1186844002_count_subject_present',\n",
    " '1186844002_count_relative_present',\n",
    " 'Idiopathic hemochromatosis (disorder)_count',\n",
    " 'Idiopathic hemochromatosis (disorder)_count_subject_present',\n",
    " 'Idiopathic hemochromatosis (disorder)_count_relative_present',\n",
    " 'Neonatal hemochromatosis (disorder)_count',\n",
    " 'Neonatal hemochromatosis (disorder)_count_subject_present',\n",
    " 'Neonatal hemochromatosis (disorder)_count_relative_present',\n",
    " 'Primary hemochromatosis (disorder)_count',\n",
    " 'Primary hemochromatosis (disorder)_count_subject_present',\n",
    " 'Primary hemochromatosis (disorder)_count_relative_present',\n",
    " 'Secondary hemochromatosis (disorder)_count',\n",
    " 'Secondary hemochromatosis (disorder)_count_subject_present',\n",
    " 'Secondary hemochromatosis (disorder)_count_relative_present',\n",
    " 'African nutritional hemochromatosis (disorder)_count',\n",
    " 'African nutritional hemochromatosis (disorder)_count_subject_present',\n",
    " 'African nutritional hemochromatosis (disorder)_count_relative_present',\n",
    " 'Erythropoietic hemochromatosis (disorder)_count',\n",
    " 'Erythropoietic hemochromatosis (disorder)_count_subject_present',\n",
    " 'Erythropoietic hemochromatosis (disorder)_count_relative_present',\n",
    " 'Hemochromatosis following repeated red blood cell transfusion (disorder)_count',\n",
    " 'Hemochromatosis following repeated red blood cell transfusion (disorder)_count_subject_present',\n",
    " 'Hemochromatosis following repeated red blood cell transfusion (disorder)_count_relative_present',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count_subject_present',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count_relative_present',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count_subject_present',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count_relative_present',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count_subject_present',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count_relative_present',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count_subject_present',\n",
    " 'Homozygous For The C . 845G A P . Cys282Tyr Variant_count_relative_present',\n",
    " 'Homozygous For The C2828Y Mutation Of The Hfe Gene_count',\n",
    " 'Homozygous For The C2828Y Mutation Of The Hfe Gene_count_subject_present',\n",
    " 'Homozygous For The C2828Y Mutation Of The Hfe Gene_count_relative_present',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count_subject_present',\n",
    " 'Hfe Ph63D Heterozygous Normal Variant_count_relative_present',\n",
    " 'Homozygous For The H63D Mutation_count',\n",
    " 'Homozygous For The H63D Mutation_count_subject_present',\n",
    " 'Homozygous For The H63D Mutation_count_relative_present',\n",
    " 'This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 282 C Y This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 63 H D_count',\n",
    " 'This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 282 C Y This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 63 H D_count_subject_present',\n",
    " 'This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 282 C Y This Patient Is A Carrier For The Mutation In The Hfe Gene At Position 63 H D_count_relative_present',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count_subject_present',\n",
    " 'Homozygous For Haemochromatosis Gene Mutation_count_relative_present',\n",
    " 'Heterozygous For The Haemochromatosis Gene_count',\n",
    " 'Heterozygous For The Haemochromatosis Gene_count_subject_present',\n",
    " 'Heterozygous For The Haemochromatosis Gene_count_relative_present',\n",
    " 'Haemochromatosis gene screening test (observable entity)_count',\n",
    " 'Haemochromatosis gene screening test (observable entity)_count_subject_present',\n",
    " 'Haemochromatosis gene screening test (observable entity)_count_relative_present',\n",
    " 'Carrier of hemochromatosis (finding)_count',\n",
    " 'Carrier of hemochromatosis (finding)_count_subject_present',\n",
    " 'Carrier of hemochromatosis (finding)_count_relative_present',\n",
    " 'Hemochromatosis gene screening test (procedure)_count',\n",
    " 'Hemochromatosis gene screening test (procedure)_count_subject_present',\n",
    " 'Hemochromatosis gene screening test (procedure)_count_relative_present',\n",
    " 'Iron overload (disorder)_count',\n",
    " 'Iron overload (disorder)_count_subject_present',\n",
    " 'Iron overload (disorder)_count_relative_present',\n",
    " 'Family history of hemochromatosis (situation)_count',\n",
    " 'Family history of hemochromatosis (situation)_count_subject_present',\n",
    " 'Family history of hemochromatosis (situation)_count_relative_present',\n",
    "\n",
    "'Hemochromatosis gene screening test (procedure)_count_subject_present',\n",
    "'Hemochromatosis gene screening test (procedure)_count_relative_not_present',\n",
    "'Hemochromatosis gene screening test (procedure)_count',\n",
    "'Hemochromatosis gene screening test (procedure)_count_subject_not_present',\n",
    "'Hemochromatosis (disorder)_count_mrc_cs',\n",
    "'Hemochromatosis (disorder)_count_relative_not_present_mrc_cs',\n",
    "'Hemochromatosis (disorder)_count_subject_not_present_mrc_cs',\n",
    "'Hemochromatosis (disorder)_count_subject_present_mrc_cs',\n",
    "'Neonatal hemochromatosis (disorder)_count_subject_present',\n",
    "'Neonatal hemochromatosis (disorder)_count',\n",
    "'Neonatal hemochromatosis (disorder)_count_subject_not_present',\n",
    "'Neonatal hemochromatosis (disorder)_count_relative_not_present',\n",
    "'Family history of hemochromatosis (situation)_count_relative_not_present',\n",
    "'Family history of hemochromatosis (situation)_count_subject_not_present',\n",
    "'Family history of hemochromatosis (situation)_count',\n",
    "'Family history of hemochromatosis (situation)_count_subject_present',\n",
    "'Secondary hemochromatosis (disorder)_count',\n",
    "'Secondary hemochromatosis (disorder)_count_relative_not_present',\n",
    "'Secondary hemochromatosis (disorder)_count_subject_present',\n",
    "'Secondary hemochromatosis (disorder)_count_subject_not_present',\n",
    "'Carrier of hemochromatosis (finding)_count_subject_not_present_mrc_cs',\n",
    "'Carrier of hemochromatosis (finding)_count_subject_present_mrc_cs',\n",
    "'Carrier of hemochromatosis (finding)_count_relative_not_present_mrc_cs',\n",
    "'Carrier of hemochromatosis (finding)_count_mrc_cs',\n",
    "'Hereditary hemochromatosis (disorder)_count_subject_not_present_mrc_cs',\n",
    "'Hereditary hemochromatosis (disorder)_count_mrc_cs',\n",
    "'Hereditary hemochromatosis (disorder)_count_subject_present_mrc_cs',\n",
    "'Hereditary hemochromatosis (disorder)_count_relative_not_present_mrc_cs',\n",
    "'Homozygote For The C282Y Mutation_count_relative_not_present',\n",
    "'Homozygote For The C282Y Mutation_count_subject_present',\n",
    "'Homozygote For The C282Y Mutation_count_subject_not_present',\n",
    "'Homozygote For The C282Y Mutation_count'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state_val)\n",
    "#metric_list = [\"accuracy\", \"roc_auc\", \"f1\", \"precision\", \"recall\"]\n",
    "\n",
    "metric_list = {'auc': make_scorer(roc_auc_score, needs_proba=False),\n",
    "                'f1':'f1',\n",
    "                'accuracy':'accuracy',\n",
    "                'recall': 'recall'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(X_train, Y_train, x_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, sub_sample_parameter_val = 100):\n",
    "    \n",
    "    global grid_n_jobs\n",
    "    print(f\"grid searching {algorithm_implementation}\")\n",
    "    \n",
    "    gpu_model_list = ['FCNClassifier']\n",
    "    \n",
    "    ## grid n jobs overloading gpu?\n",
    "    if(algorithm_implementation in gpu_model_list ):\n",
    "        grid_n_jobs = 1\n",
    "        \n",
    "    grid_n_jobs = 1\n",
    "    gpu_id_n = get_free_gpu()\n",
    "    print(f\"gpu_id_n {gpu_id_n}\")\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id_n)\n",
    "    \n",
    "    \n",
    "    current_algorithm = algorithm_implementation()\n",
    "    \n",
    "    parameters = parameter_space\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    #Grid search over hyperparameter space, randomised. \n",
    "    if(random_grid_search):\n",
    "            #n_iter_v = int(self.sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2\n",
    "            n_iter_v = int(len(ParameterGrid(parameter_space))) + 2\n",
    "            \n",
    "            if(sub_sample_parameter_val < n_iter_v):\n",
    "                n_iter_v = sub_sample_parameter_val\n",
    "            \n",
    "            grid = RandomizedSearchCV(current_algorithm, parameters,\n",
    "                                    verbose=1, cv=[(slice(None), slice(None))],\n",
    "                                    n_jobs =grid_n_jobs, n_iter = n_iter_v, error_score=np.nan)\n",
    "    else:   \n",
    "        grid = GridSearchCV(\n",
    "            current_algorithm, parameters, verbose=1, cv=[(slice(None), slice(None))], n_jobs = grid_n_jobs,\n",
    "            error_score=np.nan\n",
    "        )\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    pg = ParameterGrid(parameter_space)\n",
    "    pg = len(pg)\n",
    "    print(pg)\n",
    "    if pg > 100000:\n",
    "        print(\"grid too large\", str(pg))\n",
    "        #raise Exception(\"grid too large\", str(pg))\n",
    "    # print(grid)\n",
    "    print(\"Full pg\", pg)\n",
    "    \n",
    "    #could sub sample X/y here in hyperparameter search. Smaller dataset == faster param search\n",
    "    if(sub_sample_hyperparameter_search):\n",
    "        X_train_sample = X_train.sample(sub_sample_hyperparameter_search_val).copy()\n",
    "        y_train_sample = y_train[X_train_sample.index].copy()\n",
    "        hp_grid_samp_len = len(X_train_sample)\n",
    "    \n",
    "        grid.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    else:\n",
    "        hp_grid_samp_len = len(X_train)\n",
    "        #grid.fit(X, y)  \n",
    "        grid.fit(X_train, y_train) \n",
    "        \n",
    "    print(f\"Fitted {grid.best_estimator_} done\")\n",
    "        \n",
    "    current_algorithm = grid.best_estimator_\n",
    "    \n",
    "    current_algorithm.fit(X_train, y_train)    \n",
    "        \n",
    "    scores = cross_validate(\n",
    "        current_algorithm,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=metric_list,\n",
    "        cv=cv,\n",
    "        n_jobs=grid_n_jobs,  # Full CV on final best model #exp -1 was 1\n",
    "        pre_dispatch = 80, #exp,\n",
    "        error_score=np.nan\n",
    "    )\n",
    "    current_algorithm_scores = scores\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(\"Writing grid permutation to log\")\n",
    "        #write line to best grid scores---------------------\n",
    "        column_list = ['algorithm_implementation', 'parameter_sample','method_name', 'nb_size', 'f_list', 'auc','mcc','f1','precision','recall','accuracy',\n",
    "                       \n",
    "               'resample', 'scale', 'n_features', 'param_space_size', 'n_unique_out',\n",
    "               'outcome_var_n', 'percent_missing', 'corr', \n",
    "                       'age', 'sex', 'bmi','ethnicity', 'bloods', 'diagnostic_order',\n",
    "                      'drug_order', 'annotation_n', 'meta_sp_annotation_n',\n",
    "               'meta_sp_annotation_mrc_n','annotation_mrc_n', 'date_time_stamp',\n",
    "               'core_02','bed','vte_status','hosp_site','core_resus','news', 'hp_grid_samp_len',\n",
    "                'X_train_size', 'X_test_orig_size', 'X_test_size', 'run_time', 'n_fits', 't_fits', 'i',\n",
    "                \n",
    "              ]\n",
    "        \n",
    "        metric_names = []\n",
    "        for metric in metric_list:\n",
    "            metric_names.append(f'{metric}_m')\n",
    "            metric_names.append(f'{metric}_std')\n",
    "            \n",
    "        column_list.extend(metric_names)\n",
    "        \n",
    "\n",
    "        #column_list = column_list +['BL_' + str(x) for x in range(0, 64)]\n",
    "\n",
    "        line = pd.DataFrame( data = None, columns = column_list)\n",
    "        \n",
    "        score_successful = False\n",
    "        \n",
    "        try:\n",
    "            best_pred_orig = current_algorithm.predict(X_test_orig) #exp\n",
    "            score_successful = True\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            print(\"failed to get best_pred_orig returning nan vector\")\n",
    "            \n",
    "            empty_vector_len = X_test_orig.shape[0]\n",
    "            \n",
    "            empty_vector = np.empty(empty_vector_len)\n",
    "\n",
    "            empty_vector[:] = np.nan\n",
    "\n",
    "            best_pred_orig = empty_vector\n",
    "            \n",
    "            score_successful = False\n",
    "        \n",
    "        \n",
    "        #best_pred_orig = grid.best_estimator_.predict(X_test_orig)\n",
    "        if(score_successful):\n",
    "            auc = metrics.roc_auc_score(y_test_orig, best_pred_orig)\n",
    "            mcc = matthews_corrcoef(y_test_orig, best_pred_orig)\n",
    "            f1  = f1_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            precision = precision_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            recall = recall_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            accuracy = accuracy_score(y_test_orig, best_pred_orig)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "            mcc = np.nan\n",
    "            f1  = np.nan\n",
    "            precision = np.nan\n",
    "            recall = np.nan\n",
    "            accuracy = np.nan\n",
    "\n",
    "        for key in global_param_dict:\n",
    "            #print(key)\n",
    "            if key != 'data':\n",
    "                if(key in column_list):\n",
    "                    line[key] = [global_param_dict.get(key)]\n",
    "            else:\n",
    "                for key_1 in global_param_dict.get('data'):\n",
    "                    #print(key_1)\n",
    "                    if(key_1 in column_list):\n",
    "                        line[key_1] = [global_param_dict.get('data').get(key_1)]\n",
    "\n",
    "        current_f = final_column_list\n",
    "        #current_f = list(X_test.columns)\n",
    "        \n",
    "        current_f_vector = []\n",
    "        f_list = []\n",
    "        for elem in orignal_feature_names:\n",
    "            if(elem in current_f):\n",
    "                current_f_vector.append(1)\n",
    "            else:\n",
    "                current_f_vector.append(0)\n",
    "        #f_list.append(np.array(current_f_vector))\n",
    "        f_list.append(current_f_vector)\n",
    "        \n",
    "        line['algorithm_implementation'] = [algorithm_implementation]\n",
    "        line['parameter_sample'] = [current_algorithm]\n",
    "        line['method_name'] = [method_name]\n",
    "        line['nb_size'] = [sum(np.array(current_f_vector))]\n",
    "        line['n_features'] = [len(current_f_vector)]\n",
    "        line['f_list'] = [f_list]\n",
    "        line['hp_grid_samp_len'] = [hp_grid_samp_len]\n",
    "\n",
    "\n",
    "        line['auc'] = [auc]\n",
    "        line['mcc'] = [mcc]\n",
    "        line['f1'] = [f1]\n",
    "        line['precision'] = [precision]\n",
    "        line['recall'] = [recall]\n",
    "        line['accuracy'] = [accuracy]\n",
    "        \n",
    "        line['X_train_size'] = [len(X_train)]\n",
    "        line['X_test_orig_size'] = [len(X_test_orig)]\n",
    "        line['X_test_size'] = [len(X_test)]\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        line['run_time'] = int((end - start) / 60)\n",
    "        line['t_fits'] = pg\n",
    "        line['n_fits'] = n_iter_v\n",
    "        line['i'] = i\n",
    "        line['fit_time_m'] = scores['fit_time'].mean()\n",
    "        line['fit_time_std'] = scores['fit_time'].std()\n",
    "        \n",
    "        line['score_time_m'] = scores['score_time'].mean()\n",
    "        line['score_time_std'] = scores['score_time'].std()\n",
    "        \n",
    "        for metric in metric_list:\n",
    "            line[f'{metric}_m'] = scores[f'test_{metric}'].mean()\n",
    "            line[f'{metric}_std'] = scores[f'test_{metric}'].std()\n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "        display(line)\n",
    "        \n",
    "        #line['outcome_var'] = y_test.name\n",
    "\n",
    "        #line['nb_val'] = [nb_val]\n",
    "        #line['pop_val'] = [pop_val]\n",
    "        #line['g_val'] = [g_val]\n",
    "        #line['g'] = [g]\n",
    "\n",
    "\n",
    "        line[column_list].to_csv(base_project_dir + 'final_grid_score_log.csv' , mode='a', header=False, index=True)   \n",
    "        #---------------------------    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Failed to upgrade grid entry\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return (method_name, current_algorithm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de557639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#necessary? fcn classifier bound\n",
    "\n",
    "#also breaks cv with numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_project_dir_global = 'HFE_ML_TS_experiments/'\n",
    "\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a50a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "base_project_dir = 'HFE_ML_TS_experiments/' + st_time + \"/\"\n",
    "additional_naming = \"HFE_ML_TS_Grid_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#init final grid scores\n",
    "column_list = ['algorithm_implementation', 'parameter_sample','method_name', 'nb_size', 'f_list', 'auc','mcc','f1','precision','recall','accuracy',        \n",
    "                 'resample', 'scale', 'n_features', 'param_space_size', 'n_unique_out',\n",
    "                 'outcome_var_n', 'percent_missing', 'corr', \n",
    "                 'age', 'sex', 'bmi','ethnicity', 'bloods', 'diagnostic_order',\n",
    "                 'drug_order', 'annotation_n', 'meta_sp_annotation_n',\n",
    "                 'meta_sp_annotation_mrc_n','annotation_mrc_n', 'date_time_stamp',\n",
    "                 'core_02','bed','vte_status','hosp_site','core_resus','news', 'hp_grid_samp_len',\n",
    "                 'X_train_size', 'X_test_orig_size', 'X_test_size', 'run_time', 'n_fits', 't_fits', 'i' \n",
    "              ]\n",
    "\n",
    "metric_names = []\n",
    "for metric in metric_list:\n",
    "    metric_names.append(f'{metric}_m')\n",
    "    metric_names.append(f'{metric}_std')\n",
    "\n",
    "column_list.extend(metric_names)\n",
    "\n",
    "#column_list = column_list +['BL_' + str(x) for x in range(0, 64)]\n",
    "\n",
    "df_scores = pd.DataFrame( data = None, columns = column_list)\n",
    "\n",
    "df_scores.to_csv(base_project_dir + 'final_grid_score_log.csv', mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "log_small = np.logspace(-1, -5, 3)\n",
    "print(log_small)\n",
    "bool_param = [True, False]\n",
    "print(bool_param)\n",
    "log_large = np.logspace(0, 2, 3).astype(int)\n",
    "print(log_large)\n",
    "log_large_long = np.floor(np.logspace(0, 2.8, 6)).astype(int)\n",
    "print(log_large_long)\n",
    "log_med_long = np.floor(np.logspace(0, 1.5, 5)).astype(int)\n",
    "print(log_med_long)\n",
    "log_med = np.floor(np.logspace(0, 1.5, 3)).astype(int)\n",
    "print(log_med)\n",
    "nstep = 3\n",
    "log_zero_one = np.logspace(0.0, 1.0, nstep) / 10\n",
    "print(log_zero_one)\n",
    "lin_zero_one = np.linspace(0.0, 1.0, nstep) / 10\n",
    "print(lin_zero_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75de27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_epoch = np.floor(np.logspace(0, 1.5, 3)).astype(int) #adjust later\n",
    "print(log_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6039f099",
   "metadata": {},
   "source": [
    "## Distance based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighborsTimeSeriesClassifier_class():\n",
    "    \n",
    "    algorithm_implementation = KNeighborsTimeSeriesClassifier\n",
    "    \n",
    "    method_name = 'KNeighborsTimeSeriesClassifier'\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        'distance' : ['dtw', 'euclidean',  ], # , 'cityblock' 'ctw', 'sqeuclidean','sax' 'softdtw'\n",
    "        'n_neighbors' :  [2, 3, 5],# [log_med_long]\n",
    "        'n_jobs': [n_jobs_model_val],\n",
    "        \n",
    "    }\n",
    "\n",
    "    #nb consider probability scoring on binary class eval: CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_combinations(items_list):\n",
    "    result = []\n",
    "    for i in range(1, len(items_list) + 1):\n",
    "        for combo in combinations(items_list, i):\n",
    "            result.append(list(combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "class ElasticEnsemble_class():\n",
    "    \n",
    "    #measures = ['euclidean', 'dtw', 'wdtw', 'ddtw', 'dwdtw', 'lcss', 'erp', 'msm', 'twe'] #none supported\n",
    "    \n",
    "    #measures = [ ]\n",
    "\n",
    "    #res = all_combinations(measures)\n",
    "    \n",
    "    #res.extend(['all'])\n",
    "    \n",
    "    algorithm_implementation = ElasticEnsemble\n",
    "    \n",
    "    method_name = 'ElasticEnsemble'\n",
    "    \n",
    "    #['euclidean', 'dtw', 'wdtw', 'ddtw', 'dwdtw', 'lcss', 'erp', 'msm', 'twe']\n",
    "    \n",
    "    parameter_space = {\n",
    "#        'distance_measures': [ 'all'],\n",
    "        #'distance_measures': [],\n",
    "        'proportion_of_param_options': [1.0, 0.8, 0.6],\n",
    "        'proportion_train_in_param_finding': [1.0, 0.8, 0.6],\n",
    "        'proportion_train_for_test': [1.0, 0.8, 0.6],\n",
    "        'n_jobs': [n_jobs_model_val],  # -1 means using all processors.\n",
    "        #'random_state': [0],  # The random seed.\n",
    "#        'verbose': [1],  # If >0, then prints out debug information. #this is throwing an error\n",
    "        'majority_vote': [False, True],  # Whether to use majority vote or weighted vote.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b96771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDTW_class():\n",
    "    \n",
    "    algorithm_implementation = ShapeDTW\n",
    "    \n",
    "    method_name = 'ShapeDTW'\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        'n_neighbours': [-1],\n",
    "        'subsequence_length': ['sqrt(n_timepoints)'],\n",
    "        'shape_descriptor_function': ['raw'],\n",
    "        'params': [None],\n",
    "        'shape_descriptor_functions': [['raw', 'derivative']],\n",
    "        'metric_params': [None],\n",
    "        'n_jobs': [n_jobs_model_val]\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9beb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVECOTEV1_class():\n",
    "    \n",
    "    algorithm_implementation = HIVECOTEV1\n",
    "    \n",
    "    method_name = 'HIVECOTEV1'\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        'stc_params': [None],\n",
    "        'tsf_params': [None],\n",
    "        'rise_params': [None],\n",
    "        'cboss_params': [None],\n",
    "        'verbose': [verbose_param],\n",
    "        'n_jobs': [n_jobs_model_val],\n",
    "        'random_state': [random_state_val]\n",
    "    }\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HIVECOTEV2_class():\n",
    "    \n",
    "#     algorithm_implementation = HIVECOTEV2\n",
    "    \n",
    "#     method_name = 'HIVECOTEV2'\n",
    "    \n",
    "#     parameter_space = {\n",
    "#         'stc_params': [None],\n",
    "#         'drcif_params': [None],\n",
    "#         'arsenal_params': [None],\n",
    "#         'tde_params': [None],\n",
    "#         'time_limit_in_minutes': [0],\n",
    "#         'save_component_probas': [False],\n",
    "#         'verbose': [0],\n",
    "#         'n_jobs': [1],\n",
    "#         'random_state': [None]\n",
    "        \n",
    "        \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44d3d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVECOTEV2_class():\n",
    "    algorithm_implementation = HIVECOTEV2\n",
    "\n",
    "    method_name = 'HIVECOTEV2'\n",
    "    \n",
    "    #NB should add multi sets of params somehow to ensemble components!\n",
    "\n",
    "    parameter_space = {\n",
    "        'stc_params': [None, ],  # Parameters for the ShapeletTransformClassifier module. If None, uses the default parameters with a 2-hour transform contract.\n",
    "        'drcif_params': [None, ],  # Parameters for the DrCIF module. If None, uses the default parameters with n_estimators set to 500.\n",
    "        'arsenal_params': [None, ],  # Parameters for the Arsenal module. If None, uses the default parameters.\n",
    "        'tde_params': [None, ],  # Parameters for the TemporalDictionaryEnsemble module. If None, uses the default parameters.\n",
    "        'time_limit_in_minutes': time_limit_param,  # Time contract to limit build time in minutes, overriding n_estimators/n_parameter_samples for each component. Default of 0 means n_estimators/n_parameter_samples for each component is used.\n",
    "        'save_component_probas': [False],  # When predict/predict_proba is called, save each HIVE-COTEV2 component probability predictions in component_probas.\n",
    "        'verbose': [verbose_param],  # Level of output printed to the console (for information only).\n",
    "        'random_state': [random_state_val],  # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "        'n_jobs': [n_jobs_model_val],  # The number of jobs to run in parallel for both fit and predict. -1 means using all processors.\n",
    "        #'parallel_backend': [ 'multiprocessing'], #None, 'loky',  , 'threading'# Specify the parallelization backend implementation in joblib for Catch22, if None a ‘prefer’ value of “threads” is used by default. Valid options are “loky”, “multiprocessing”, “threading” or a custom backend. See the joblib Parallel documentation for more details.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RocketClassifier_class():\n",
    "    \n",
    "#     algorithm_implementation = RocketClassifier()\n",
    "    \n",
    "#     method_name = 'RocketClassifier'\n",
    "    \n",
    "#     parameter_space = {\n",
    "        \n",
    "#         'num_kernels' : [10000], # , 'cityblock' 'ctw', 'sqeuclidean','sax' 'softdtw'\n",
    "#         'rocket_transform' :  ['rocket', 'minirocket', 'multirocket']# [log_med_long],,\n",
    "        \n",
    "#         #'max_dilations_per_kernel' : []\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocketClassifier_class():\n",
    "    algorithm_implementation = RocketClassifier\n",
    "\n",
    "    method_name = 'RocketClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'num_kernels': [5000, 10000, 15000],  # The number of kernels for the Rocket transform.\n",
    "        'rocket_transform': [\"rocket\", \"minirocket\", \"multirocket\"],  # The type of Rocket transformer to use. Valid inputs = [\"rocket\", \"minirocket\", \"multirocket\"].\n",
    "        'max_dilations_per_kernel': [16, 32, 64],  # MiniRocket and MultiRocket only. The maximum number of dilations per kernel.\n",
    "        'n_features_per_kernel': [3, 4, 5],  # MultiRocket only. The number of features per kernel.\n",
    "        'random_state': [random_state_val],  # Seed for random number generation.\n",
    "        'estimator': [None],  # If none, a RidgeClassifierCV(alphas=np.logspace(-3, 3, 10)) is used.\n",
    "        'n_jobs': [n_jobs_model_val],  # Number of threads to use for the convolutional transform. -1 means using all processors.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d24b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier_class():\n",
    "    algorithm_implementation = CNNClassifier\n",
    "    \n",
    "    method_name = 'CNNClassifier'\n",
    "    \n",
    "    #nb revisit params dependent on width of the dataset\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        #'n_layers': [2, 3, 4],\n",
    "        #'kernel_size': [3, 5, 7],\n",
    "        #'n_filters': [[6, 12], [8, 16], [10, 20]],\n",
    "        #'avg_pool_size': [2, 3, 4],\n",
    "        'activation': ['sigmoid', 'relu'],\n",
    "        'padding': ['valid'],\n",
    "        #'strides': [1, 2],\n",
    "        'dilation_rate': [1, 2],\n",
    "        'use_bias': [True],\n",
    "        'random_state': [random_state_val],\n",
    "        'n_epochs': log_epoch,\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'verbose': [ verbose_param],\n",
    "        'loss': ['binary_crossentropy'],\n",
    "        'metrics': ['accuracy'],\n",
    "        #'save_best_model': [True, False],\n",
    "        #'save_last_model': [True, False],\n",
    "        #'best_file_name': ['best_model', 'top_model'],\n",
    "        #'last_file_name': ['last_model', 'final_model'], \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d3704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNClassifier_class():\n",
    "    algorithm_implementation = FCNClassifier\n",
    "    \n",
    "    method_name = 'FCNClassifier'\n",
    "    \n",
    "    parameter_space = {\n",
    "        'n_layers': [3],\n",
    "        'n_filters': [128, 256, 128],\n",
    "        'kernel_size': [8, 5, 3],\n",
    "        'dilation_rate': [1],\n",
    "        'strides': [1],\n",
    "        'padding': ['same'],\n",
    "        'activation': ['relu'],\n",
    "        'use_bias': [True],\n",
    "        'n_epochs': log_epoch,\n",
    "        'batch_size': [16],\n",
    "        'use_mini_batch_size': [True],\n",
    "        'random_state': [random_state_val],\n",
    "        'verbose': [verbose_param],\n",
    "        'loss': ['categorical_crossentropy'],\n",
    "        'metrics': [None],\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)], \n",
    "        #'n_jobs':[1] #not a param\n",
    "        #'file_path': ['./'],\n",
    "        #'save_best_model': [False],\n",
    "        #'save_last_model': [False],\n",
    "        #'best_file_name': ['best_model'],\n",
    "        #'last_file_name': ['last_model'],\n",
    "        #'callbacks': [None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d41d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TapNetClassifier_class():\n",
    "#     algorithm_implementation = TapNetClassifier\n",
    "    \n",
    "#     method_name = 'TapNetClassifier'\n",
    "    \n",
    "#     parameter_space = {\n",
    "#         'filter_sizes': [(256, 256, 128)],\n",
    "#         'kernel_sizes': [(8, 5, 3)],\n",
    "#         'layers': [(500, 300)],\n",
    "#         'reduction': [16],\n",
    "#         'n_epochs': [2000],\n",
    "#         'batch_size': [16],\n",
    "#         'dropout': [0.5],\n",
    "#         'dilation': [1],\n",
    "#         'activation': ['sigmoid'],\n",
    "#         'loss': ['binary_crossentropy'],\n",
    "#         #'optimizer': ['Adam(lr=0.01)'],\n",
    "#         'use_bias': [True],\n",
    "#         'use_rp': [True],\n",
    "#         'use_att': [True],\n",
    "#         'use_lstm': [True],\n",
    "#         'use_cnn': [True],\n",
    "#         'verbose': [False],\n",
    "#         'random_state': [None]\n",
    "#     }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1708379",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TapNetClassifier_class():\n",
    "    algorithm_implementation = TapNetClassifier\n",
    "\n",
    "    method_name = 'TapNetClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'filter_sizes': [(256, 256, 128), (128, 128, 64)],  # Sets the kernel size argument for each convolutional block. Controls the number of convolutional filters and the number of neurons in attention dense layers.\n",
    "        'kernel_size': [(8, 5, 3), (4, 3, 2)],  # Controls the size of the convolutional kernels.\n",
    "        'layers': [(500, 300), (400, 200)],  # Size of dense layers.\n",
    "        #'reduction': [16, 32],  # Divides the number of dense neurons in the first layer of the attention block.\n",
    "        'n_epochs': log_epoch,  # Number of epochs to train the model.\n",
    "        'batch_size': [16, 32],  # Number of samples per update.\n",
    "        'dropout': [0.5, 0.3, 0.2],  # Dropout rate, in the range [0, 1).\n",
    "        'dilation': [1, 2],  # Dilation value.\n",
    "        'activation': ['sigmoid', 'relu'],  # Activation function for the last output layer.\n",
    "        'loss': ['binary_crossentropy', 'categorical_crossentropy'],  # Loss function for the classifier.\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)],  # Gradient updating function for the classifier.\n",
    "        'use_bias': [True, False],  # Whether to use bias in the output dense layer.\n",
    "        'use_rp': [True, False],  # Whether to use random projections.\n",
    "        'use_att': [True, False],  # Whether to use self-attention.\n",
    "        'use_lstm': [True, False],  # Whether to use an LSTM layer.\n",
    "        'use_cnn': [True, False],  # Whether to use a CNN layer.\n",
    "        'verbose': [ verbose_param],  # Whether to output extra information.\n",
    "        'random_state': [random_state_val],  # Seed for random.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5ab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Arsenal_class():\n",
    "#     algorithm_implementation = Arsenal()\n",
    "    \n",
    "#     method_name = 'Arsenal'\n",
    "    \n",
    "#     parameter_space = {\n",
    "#         'num_kernels': [2000],\n",
    "#         'n_estimators': [25],\n",
    "#         'rocket_transform': ['rocket'],\n",
    "#         'max_dilations_per_kernel': [32],\n",
    "#         'n_features_per_kernel': [4],\n",
    "#         'time_limit_in_minutes': [0],\n",
    "#         'contract_max_n_estimators': [100],\n",
    "#         'save_transformed_data': [False],\n",
    "#         'n_jobs': [-1],\n",
    "#         'random_state': [None]\n",
    "        \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfe176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arsenal_class():\n",
    "    algorithm_implementation = Arsenal\n",
    "\n",
    "    method_name = 'Arsenal'\n",
    "\n",
    "    parameter_space = {\n",
    "         'num_kernels': [1000, 2000, 3000],  # Number of kernels for each ROCKET transform.\n",
    "        'n_estimators': [3, 5, 6],  # Number of estimators to build for the ensemble.\n",
    "        'rocket_transform': [\"rocket\", \"minirocket\"],  # The type of Rocket transformer to use. #, \"multirocket\" # broken\n",
    "        # Valid inputs = [\"rocket\", \"minirocket\", \"multirocket\"].\n",
    "        'max_dilations_per_kernel': [16, 32, 64],  # MiniRocket and MultiRocket only. The maximum number of dilations per kernel.\n",
    "        'n_features_per_kernel': [3, 4, 5],  # MultiRocket only. The number of features per kernel.\n",
    "        'time_limit_in_minutes': time_limit_param,  # Time contract to limit build time in minutes, overriding n_estimators. Default of 0 means n_estimators is used.\n",
    "        'contract_max_n_estimators': [50, 100, 150],  # Max number of estimators when time_limit_in_minutes is set.\n",
    "        #'save_transformed_data': [True, False],  # Save the data transformed in fit for use in _get_train_probs.\n",
    "        'n_jobs': [n_jobs_model_val],  # The number of jobs to run in parallel for both fit and predict. -1 means using all processors.\n",
    "        'random_state': [random_state_val],  # Seed for random number generation.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622af825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualTDE_class():\n",
    "    algorithm_implementation = IndividualTDE\n",
    "\n",
    "    method_name = 'IndividualTDE'\n",
    "\n",
    "    parameter_space = {\n",
    "        'window_size': [5, 10, 15],\n",
    "        'word_length': [4, 8, 12],\n",
    "        'norm': [True, False],\n",
    "        'levels': [1, 2, 3],\n",
    "        'igb': [True, False],\n",
    "        'alphabet_size': [3, 4, 5],\n",
    "        'bigrams': [True, False],\n",
    "        'dim_threshold': [0.8, 0.85, 0.9],\n",
    "        'max_dims': [15, 20, 25],\n",
    "        'typed_dict': [True, False],\n",
    "        'n_jobs': [n_jobs_model_val],\n",
    "        'random_state': [random_state_val],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c193f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #can't find this in the API\n",
    "\n",
    "# class CanonicalIntervalForest_class():\n",
    "#     algorithm_implementation = CanonicalIntervalForest\n",
    "\n",
    "#     method_name = 'CanonicalIntervalForest'\n",
    "\n",
    "#     parameter_space = {\n",
    "#         'n_estimators': [100, 200, 300],              # Number of estimators to build for the ensemble\n",
    "#         'n_intervals': [10, 20, 30],                  # Number of intervals to extract per tree\n",
    "#         'att_subsample_size': [4, 8, 12],             # Number of catch22 or summary statistic attributes to subsample per tree\n",
    "#         'min_interval': [2, 3, 4],                    # Minimum length of an interval\n",
    "#         'max_interval': [None, 10, 20],               # Maximum length of an interval\n",
    "#         'base_estimator': [CanonicalIntervalForest()],# Base estimator for the ensemble\n",
    "#         'n_jobs': [-1],                           # The number of jobs to run in parallel for both `fit` and `predict`\n",
    "#         'random_state': [0],                   # Seed for random number generation\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Catch22Classifier_class():\n",
    "    algorithm_implementation = Catch22Classifier\n",
    "\n",
    "    method_name = 'Catch22Classifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'features': [\"all\", [\"DN_HistogramMode_5\", \"DN_HistogramMode_10\"], ...],  # List of catch22 features to extract\n",
    "        'catch24': [True, False],                                                  # Extract mean, std, and 22 Catch22 features\n",
    "        'outlier_norm': [True, False],                                             # Normalize during outlier Catch22 features\n",
    "        'replace_nans': [True, False],                                             # Replace NaN/inf values from the transform\n",
    "        'use_pycatch22': [True, False],                                            # Use C-based pycatch22 implementation\n",
    "        'estimator': [RandomForestClassifier(n_estimators=200),                  # Sklearn estimator for building the model\n",
    "                      DecisionTreeClassifier()],                              # Add more estimators if desired\n",
    "        'random_state': [random_state_val],                                                # Random seed for random number generator\n",
    "        'n_jobs': [n_jobs_model_val],                                                         # Number of jobs for parallel processing\n",
    "       # 'parallel_backend': [None, \"loky\", \"multiprocessing\", \"threading\"],        # Parallelization backend options\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U aeon[dl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202b1a3",
   "metadata": {},
   "source": [
    "dict_keys(['att_subsample_size', 'base_estimator', 'contract_max_n_estimators', 'max_interval_length', 'min_interval_length', 'n_estimators', 'n_intervals', 'n_jobs', 'parallel_backend', 'random_state', 'save_transformed_data', 'time_limit_in_minutes', 'use_pycatch22', 'use_pyfftw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f09af6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#warning complex method\n",
    "class DrCIF_class():\n",
    "    algorithm_implementation = DrCIFClassifier\n",
    "\n",
    "    method_name = 'DrCIF'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_estimators': [100, 200, 300],              # Number of estimators to build for the ensemble\n",
    "        'n_intervals': [10, 20, 30],                  # Number of intervals to extract per representation per tree\n",
    "        'att_subsample_size': [4, 8, 12],             # Number of catch22 or summary statistic attributes to subsample per tree\n",
    "        'min_interval_length': [2, 3, 4],                    # Minimum length of an interval per representation\n",
    "        'max_interval_length': [10, 20],               # Maximum length of an interval per representation\n",
    "        'base_estimator': [  # Base estimator for the ensemble\n",
    "                           None],             # Add more base estimator options if desired #defaults to tree \n",
    "        'time_limit_in_minutes': time_limit_param,         # Time contract to limit build time in minutes\n",
    "        'contract_max_n_estimators': [100, 500, 1000],# Max number of estimators when time_limit_in_minutes is set\n",
    "        #'save_transformed_data': [True, False],       # Save the transformed dataset for use in _get_train_probs\n",
    "        'n_jobs': [n_jobs_model_val],                           # Number of jobs for parallel processing\n",
    "        'random_state': [random_state_val],                   # Random seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c9645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDictionaryEnsemble_class():\n",
    "    algorithm_implementation = aeon.classification.dictionary_based._tde.TemporalDictionaryEnsemble\n",
    "    \n",
    "\n",
    "    method_name = 'TemporalDictionaryEnsemble'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_parameter_samples': [100, 250, 500],          # Number of parameter combinations to consider\n",
    "        'max_ensemble_size': [25, 50, 100],              # Maximum number of estimators in the ensemble\n",
    "        'max_win_len_prop': [0.5, 1.0],                 # Maximum window length as a proportion of series length\n",
    "        'min_window': [5, 10, 15],                      # Minimum window length\n",
    "        'randomly_selected_params': [25, 50, 75],       # Number of randomly selected parameters before GP parameter selection\n",
    "        'bigrams': [True, False, None],                 # Whether to use bigrams\n",
    "        'dim_threshold': [0.7, 0.85, 0.95],             # Dimension accuracy threshold for multivariate data\n",
    "        'max_dims': [10, 20, 30],                       # Max number of dimensions per classifier for multivariate data\n",
    "        'time_limit_in_minutes': time_limit_param,           # Time contract to limit build time in minutes\n",
    "        'contract_max_n_parameter_samples': [100, 250, 500],  # Max number of parameter combinations to consider with time limit\n",
    "        'typed_dict': [True, False],                    # Use a numba typed Dict to store word counts\n",
    "        #'save_train_predictions': [True, False],        # Save the ensemble member train predictions in fit\n",
    "        'n_jobs': [n_jobs_model_val],                             # Number of jobs for parallel processing\n",
    "        'random_state': [random_state_val],                     # Random seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a1bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
    "# The versions of TensorFlow you are currently using is 2.9.1 and is not supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0186474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderClassifier_class():\n",
    "    algorithm_implementation = EncoderClassifier\n",
    "\n",
    "    method_name = 'EncoderClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'kernel_size': [[5], [11], [21], [5, 11], [5, 21], [11, 21], [5, 11, 21]],  # Specifying the length of the 1D convolution windows\n",
    "        'n_filters': [[128], [256], [512], [128, 256], [128, 512], [256, 512], [128, 256, 512]],  # Specifying the number of 1D convolution filters used for each layer\n",
    "        'max_pool_size': [2, 3],                          # Size of the max pooling windows\n",
    "        'activation': ['sigmoid', 'relu', 'tanh'],        # Keras activation function\n",
    "        'dropout_proba': [0.0, 0.2, 0.5],                 # Dropout layer probability\n",
    "        'padding': ['same', 'valid'],                     # Type of padding used for 1D convolution\n",
    "        'strides': [1, 2],                               # Sliding rate of the 1D convolution filter\n",
    "        'fc_units': [128, 256, 512],                      # Number of units in the hidden fully connected layer\n",
    "        #'file_path': ['./', './models/'],                # File path when saving the model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'best_classifier'],  # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'last_classifier'],  # Name of the file of the last model\n",
    "        'random_state': [random_state_val],                         # Seed for any needed random actions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier_class():\n",
    "    algorithm_implementation = ResNetClassifier\n",
    "\n",
    "    method_name = 'ResNetClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_residual_blocks': [2, 3, 4],            # The number of residual blocks of ResNet's model\n",
    "        'n_conv_per_residual_block': [2, 3, 4],     # The number of convolution blocks in each residual block\n",
    "        'n_filters': [64, 128, 256],               # The number of convolution filters for all the convolution layers in the same residual block\n",
    "        #'kernel_sizes': [3, 5, 7],                 # The kernel size of all the convolution layers in one residual block\n",
    "        'strides': [1, 2],                         # The strides of convolution kernels in each of the convolution layers in one residual block\n",
    "        'dilation_rate': [1, 2],                   # The dilation rate of the convolution layers in one residual block\n",
    "        'padding': ['same', 'valid'],              # The type of padding used in the convolution layers in one residual block\n",
    "        'activation': ['relu', 'tanh'],            # Keras activation used in the convolution layers in one residual block\n",
    "        'use_bias': [True, False],                 # Condition on whether or not to use bias values in the convolution layers in one residual block\n",
    "        'n_epochs': log_epoch,            # The number of epochs to train the model\n",
    "        'batch_size': [16, 32, 64],                # The number of samples per gradient update\n",
    "        'use_mini_batch_size': [True, False],      # Condition on using the mini-batch size formula Wang et al.\n",
    "        'callbacks': [None],                       # List of tf.keras.callbacks.Callback objects\n",
    "        #'file_path': ['./', './models/'],          # File path when saving model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'best_classifier'],  # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'last_classifier'],  # Name of the file of the last model\n",
    "        'verbose': [verbose_param],                  # Whether to output extra information\n",
    "        'loss': [ 'categorical_crossentropy'],  # Fit parameter for the keras model # 'mean_squared_error',\n",
    "        'optimizer': [keras.optimizers.Adadelta(), keras.optimizers.Adam()],                  # Optimizer for the model\n",
    "        'metrics': ['accuracy', 'mae'],  # List of strings for metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreshPRINCEClassifier_class():\n",
    "    algorithm_implementation = FreshPRINCEClassifier\n",
    "\n",
    "    method_name = 'FreshPRINCEClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'default_fc_parameters': ['minimal', 'efficient', 'comprehensive'],  # Set of TSFresh features to be extracted\n",
    "        'n_estimators': [100, 200, 300],                 # Number of estimators for the RotationForestClassifier ensemble\n",
    "        'save_transformed_data': [False],          # Whether to save the transformed data\n",
    "        'verbose': [verbose_param],                            # Level of output printed to the console (for information only)\n",
    "        'n_jobs': [n_jobs_model_val],                              # Number of jobs for parallel processing\n",
    "        'chunksize': [None, 100, 200],                   # Number of series processed in each parallel TSFresh job\n",
    "        'random_state': [random_state_val],                 # Seed for random, integer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db6110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionTimeClassifier_class():\n",
    "    algorithm_implementation = InceptionTimeClassifier\n",
    "\n",
    "    method_name = 'InceptionTimeClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_classifiers': [3, 5, 7],               # Number of Inception models used for the Ensemble\n",
    "        'depth': [4, 6, 8],                       # Number of inception modules used\n",
    "        'nb_filters': [32, 64, 128],              # Number of filters used in one inception module\n",
    "        'nb_conv_per_layer': [3, 4],              # Number of convolution layers in each inception module\n",
    "        'kernel_size': [30, 40, 50],              # Head kernel size used for each inception module\n",
    "#         'use_max_pooling': [True, False],         # Whether to use max pooling layer in inception modules #will throw error\n",
    "        'max_pool_size': [2, 3],                  # Size of the max pooling layer\n",
    "        'strides': [1, 2],                        # Strides of kernels in convolution layers for each inception module\n",
    "        'dilation_rate': [1, 2],                  # Dilation rate of convolutions in each inception module\n",
    "        'padding': ['same', 'valid'],             # Type of padding used for convolution for each inception module\n",
    "        'activation': ['relu', 'tanh'],           # Activation function used in each inception module\n",
    "        'use_bias': [True],                # Whether to use bias values in each inception module\n",
    "        'use_residual': [True, False],            # Whether to use residual connections all over Inception\n",
    "        'use_bottleneck': [True, False],          # Whether to use bottlenecks all over Inception\n",
    "        'bottleneck_size': [16, 32],              # Bottleneck size in case use_bottleneck = True\n",
    "        'use_custom_filters': [True, False],      # Whether to use custom filters in the first inception module\n",
    "        'batch_size': [32, 64],                   # Number of samples per gradient update\n",
    "        'use_mini_batch_size': [True, False],     # Whether to use the mini batch size formula Wang et al.\n",
    "        'n_epochs': log_epoch,           # Number of epochs to train the model\n",
    "        'callbacks': [None],                      # List of tf.keras.callbacks.Callback objects\n",
    "        #'file_path': ['./'],                      # File path when saving model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model'],         # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model'],         # Name of the file of the last model\n",
    "        'random_state': [random_state_val],                  # Seed for random actions\n",
    "        'verbose': [ verbose_param],                 # Whether to output extra information\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)],                    # Keras optimizer\n",
    "        'loss': ['categorical_crossentropy'],     # Keras loss\n",
    "        'metrics': ['accuracy'],            # Keras metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looped model test on small sample needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unknown if ts class model\n",
    "class OrdinalTDE_class():\n",
    "    algorithm_implementation = OrdinalTDE\n",
    "\n",
    "    method_name = 'OrdinalTDE'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_parameter_samples': [100, 250, 500],                # Number of parameter combinations to consider for the final ensemble\n",
    "        'max_ensemble_size': [30, 50, 100],                     # Maximum number of estimators in the ensemble\n",
    "        'max_win_len_prop': [0.8, 1.0],                        # Maximum window length as a proportion of series length\n",
    "        'min_window': [5, 10, 15],                             # Minimum window length\n",
    "        'randomly_selected_params': [30, 50, 70],              # Number of parameters randomly selected before Gaussian process parameter selection\n",
    "        'bigrams': [True, False, None],                        # Whether to use bigrams\n",
    "        'dim_threshold': [0.75, 0.85, 0.95],                   # Dimension accuracy threshold for multivariate data\n",
    "        'max_dims': [10, 20, 30],                              # Max number of dimensions per classifier for multivariate data\n",
    "        'time_limit_in_minutes': time_limit_param,                  # Time contract to limit build time in minutes\n",
    "        'contract_max_n_parameter_samples': [1000, 2000],      # Max number of parameter combinations when time_limit_in_minutes is set\n",
    "        'typed_dict': [True, False],                           # Whether to use numba typed Dict to store word counts\n",
    "        #'save_train_predictions': [True, False],               # Save ensemble member train predictions in fit for LOOCV\n",
    "        'n_jobs': [n_jobs_model_val],                                     # Number of jobs to run in parallel for fit and predict\n",
    "        'random_state': [0],                              # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c43599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPClassifier_class():\n",
    "    algorithm_implementation = MLPClassifier\n",
    "\n",
    "    method_name = 'MLPClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_epochs': log_epoch,                         # Number of epochs to train the model\n",
    "        'batch_size': [8, 16, 32],                              # Number of samples per gradient update\n",
    "        'random_state': [random_state_val],                                # Seed for random number generation\n",
    "        'verbose': [verbose_param],                               # Whether to output extra information\n",
    "        'loss': ['binary_crossentropy'],  # Fit parameter for the Keras model #must be binary? # 'mean_squared_error', \n",
    "        #'file_path': ['./', '/models'],                         # File path when saving ModelCheckpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'top_model'],          # The name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'final_model'],        # The name of the file of the last model\n",
    "        'optimizer': [keras.optimizers.Adadelta(), keras.optimizers.Adam()],  # Keras optimizer\n",
    "        'metrics': [['accuracy'], ['accuracy', 'mae']],         # List of strings for metrics\n",
    "        'activation': ['sigmoid', 'relu'],                      # Activation function used in the output linear layer\n",
    "        'use_bias': [True, False],                              # Whether the layer uses a bias vector\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db889ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warn, is mv ts classifier?\n",
    "class ContractableBOSS_class():\n",
    "    algorithm_implementation = ContractableBOSS\n",
    "\n",
    "    method_name = 'ContractableBOSS'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_parameter_samples': [100, 250, 500],                # Number of parameter combos to try\n",
    "        'max_ensemble_size': [30, 50, 100],                    # Maximum number of classifiers to retain\n",
    "        'max_win_len_prop': [0.8, 1.0],                       # Maximum window length as a proportion of series length\n",
    "        'min_window': [5, 10, 15],                            # Minimum window size\n",
    "        'time_limit_in_minutes': time_limit_param,                 # Time contract to limit build time in minutes\n",
    "        'contract_max_n_parameter_samples': [1000, 2000],     # Max number of parameter combos when time_limit_in_minutes is set\n",
    "        'save_train_predictions': [True, False],              # Save ensemble member train predictions in fit for LOOCV\n",
    "        'n_jobs': [n_jobs_model_val],                                    # Number of jobs to run in parallel for fit and predict\n",
    "        'feature_selection': ['chi2', 'none', 'random'],      # Sets the feature selection strategy to be used\n",
    "        'random_state': [0],                             # Seed for random integer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUSE_class():\n",
    "    algorithm_implementation = MUSE\n",
    "\n",
    "    method_name = 'MUSE'\n",
    "\n",
    "    parameter_space = {\n",
    "        'anova': [True, False],                         # If True, Fourier coefficient selection is done via a one-way ANOVA test\n",
    "        'variance': [True, False],                      # If True, Fourier coefficient selection is done via the largest variance\n",
    "        'bigrams': [True, False],                       # Whether to create bigrams of SFA words\n",
    "        'window_inc': [2, 4],                           # Increment used to determine the next window size for BoP model\n",
    "        'alphabet_size': [4, 6, 8],                     # Number of possible letters (values) for each word\n",
    "        'use_first_order_differences': [True, False],   # If True, adds the first order differences of each dimension to the data\n",
    "        'feature_selection': ['chi2', 'none', 'random'], # Sets the feature selection strategy to be used\n",
    "        'p_threshold': [0.01, 0.05, 0.1],               # P-value threshold for chi-squared test on bag-of-words\n",
    "        'support_probabilities': [True, False],         # If True, trains a LogisticRegression to support predict_proba()\n",
    "        'n_jobs': [n_jobs_model_val],                             # Number of jobs to run in parallel for fit and predict\n",
    "        'random_state': [random_state_val],                       # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc64da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSFreshClassifier_class():\n",
    "    algorithm_implementation = TSFreshClassifier\n",
    "\n",
    "    method_name = 'TSFreshClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'default_fc_parameters': ['minimal', 'efficient', 'comprehensive'],   # Set of TSFresh features to be extracted\n",
    "        'relevant_feature_extractor': [True, False],                          # Whether to remove irrelevant features using the FRESH algorithm\n",
    "        'estimator': [None, RandomForestClassifier(n_estimators=200)],       # An sklearn estimator to be built using the transformed data\n",
    "        'verbose': [verbose_param],                                                 # Level of output printed to the console\n",
    "        'n_jobs': [n_jobs_model_val],                                                    # Number of jobs to run in parallel for fit and predict\n",
    "        'chunksize': [None, 10, 100],                                         # Number of series processed in each parallel TSFresh job\n",
    "        'random_state': [random_state_val],                                              # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09868fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryClassifier_class():\n",
    "    algorithm_implementation = SummaryClassifier\n",
    "\n",
    "    method_name = 'SummaryClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'summary_functions': ['mean', 'std', 'min', 'max', 'median', 'sum', 'skew', 'kurt', 'var', 'mad', 'sem', 'nunique', 'count'],\n",
    "        'summary_quantiles': [None, [0.25, 0.5, 0.75]],  # Optional list of series quantiles to calculate\n",
    "        'estimator': [None, RandomForestClassifier(n_estimators=200)],  # An sklearn estimator to be built using the transformed data\n",
    "        'n_jobs': [n_jobs_model_val],  # Number of jobs to run in parallel for fit and predict\n",
    "        'random_state': [random_state_val],  # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualInceptionClassifier_class():\n",
    "    algorithm_implementation = IndividualInceptionClassifier\n",
    "\n",
    "    method_name = 'IndividualInceptionClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'depth': [6, 8, 10],\n",
    "        'nb_filters': [32, 64, 128],\n",
    "        'nb_conv_per_layer': [3, 4, 5],\n",
    "        'kernel_size': [30, 40, 50],\n",
    "        'use_max_pooling': [True, False],\n",
    "        'max_pool_size': [2, 3, 4],\n",
    "        'strides': [1, 2],\n",
    "        'dilation_rate': [1, 2],\n",
    "        'padding': ['same', 'valid'],\n",
    "        'activation': ['relu', 'elu'],\n",
    "        'use_bias': [True, False],\n",
    "        'use_residual': [True, False],\n",
    "        'use_bottleneck': [True, False],\n",
    "        'bottleneck_size': [16, 32, 64],\n",
    "        'use_custom_filters': [True, False],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'use_mini_batch_size': [True, False],\n",
    "        'n_epochs': log_epoch,\n",
    "        #'callbacks': [None, [ReduceOnPlateau(), ModelCheckpoint()]],\n",
    "        #'file_path': ['./', '/path/to/save'],\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'model_best'],\n",
    "        #'last_file_name': ['last_model', 'model_last'],\n",
    "        'random_state': [random_state_val],\n",
    "        'verbose': [verbose_param],\n",
    "        #'optimizer': [Adam(), RMSprop(), SGD()],\n",
    "        'loss': ['categorical_crossentropy', 'binary_crossentropy'],\n",
    "        'metrics': ['accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureClassifier_class():\n",
    "    algorithm_implementation = SignatureClassifier\n",
    "\n",
    "    method_name = 'SignatureClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "#         'estimator': [RandomForestClassifier(n_estimators=100), DecisionTreeClassifier()],\n",
    "#         'augmentation_list': [('basepoint', 'addtime'), ('addtime',)],\n",
    "#         'window_name': ['dyadic', 'sliding', 'expanding'],\n",
    "#         'window_depth': [2, 3, 4],\n",
    "#         'window_length': [None, 50, 100],\n",
    "#         'window_step': [None, 1, 5],\n",
    "#         'rescaling': [None, 'standard', 'min-max'],\n",
    "#         'sig_tfm': ['signature', 'logsignature'],\n",
    "#         'depth': [3, 4, 5],\n",
    "        'random_state': [0],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea34ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aeon.classification.compose._channel_ensemble import ChannelEnsembleClassifier\n",
    "# #nb this is some kind of wrapper that takes other estimators as arguments. complex func arg and grid\n",
    "# class ChannelEnsembleClassifier_class():\n",
    "#     algorithm_implementation = ChannelEnsembleClassifier(estimators=[None])\n",
    "    \n",
    "#     method_name = 'ChannelEnsembleClassifier'\n",
    "    \n",
    "#     parameter_space = {\n",
    "        \n",
    "        \n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9307a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca9a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7edac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f47fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade46994",
   "metadata": {},
   "outputs": [],
   "source": [
    "[KNeighborsTimeSeriesClassifier_class,\n",
    "ElasticEnsemble_class,\n",
    "ShapeDTW_class,\n",
    "HIVECOTEV1_class,\n",
    "HIVECOTEV2_class,\n",
    "RocketClassifier_class,\n",
    "CNNClassifier_class,\n",
    "FCNClassifier_class,\n",
    "MLPClassifier_class,\n",
    "TapNetClassifier_class,\n",
    "Arsenal_class,\n",
    "IndividualTDE_class,\n",
    "#CanonicalIntervalForest_class,\n",
    "Catch22Classifier_class,\n",
    "DrCIF_class,\n",
    "TemporalDictionaryEnsemble_class,\n",
    "EncoderClassifier_class,\n",
    "ResNetClassifier_class,\n",
    "FreshPRINCEClassifier_class,\n",
    "InceptionTimeClassifier_class,\n",
    "OrdinalTDE_class,\n",
    "MLPClassifier_class,\n",
    "ContractableBOSS_class,\n",
    "MUSE_class,\n",
    "TSFreshClassifier_class,\n",
    "SummaryClassifier_class,\n",
    "SignatureClassifier_class,\n",
    "IndividualInceptionClassifier_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [('Arsenal', aeon.classification.convolution_based._arsenal.Arsenal),\n",
    "#  ('CNNClassifier', aeon.classification.deep_learning.cnn.CNNClassifier),\n",
    "#  ('CanonicalIntervalForest',\n",
    "#   aeon.classification.interval_based._cif.CanonicalIntervalForest),\n",
    "#  ('Catch22Classifier',\n",
    "#   aeon.classification.feature_based._catch22_classifier.Catch22Classifier),\n",
    "#  ('ChannelEnsembleClassifier',\n",
    "#   aeon.classification.compose._channel_ensemble.ChannelEnsembleClassifier),\n",
    "#  ('DrCIF', aeon.classification.interval_based._drcif.DrCIF),\n",
    "#  ('DummyClassifier', aeon.classification._dummy.DummyClassifier),\n",
    "#  ('EncoderClassifier',\n",
    "#   aeon.classification.deep_learning.encoder.EncoderClassifier),\n",
    "#  ('FCNClassifier', aeon.classification.deep_learning.fcn.FCNClassifier),\n",
    "#  ('FreshPRINCE', aeon.classification.feature_based._fresh_prince.FreshPRINCE),\n",
    "#  ('HIVECOTEV2', aeon.classification.hybrid._hivecote_v2.HIVECOTEV2),\n",
    "#  ('InceptionTimeClassifier',\n",
    "#   aeon.classification.deep_learning.inception_time.InceptionTimeClassifier),\n",
    "#  ('IndividualInceptionClassifier',\n",
    "#   aeon.classification.deep_learning.inception_time.IndividualInceptionClassifier),\n",
    "#  ('IndividualTDE', aeon.classification.dictionary_based._tde.IndividualTDE),\n",
    "#  ('KNeighborsTimeSeriesClassifier',\n",
    "#   aeon.classification.distance_based._time_series_neighbors.KNeighborsTimeSeriesClassifier),\n",
    "#  ('MLPClassifier', aeon.classification.deep_learning.mlp.MLPClassifier),\n",
    "#  ('MUSE', aeon.classification.dictionary_based._muse.MUSE),\n",
    "#  ('RandomIntervalClassifier',\n",
    "#   aeon.classification.interval_based._random_interval_classifier.RandomIntervalClassifier),\n",
    "#  ('ResNetClassifier',\n",
    "#   aeon.classification.deep_learning.resnet.ResNetClassifier),\n",
    "#  ('RocketClassifier',\n",
    "#   aeon.classification.convolution_based._rocket_classifier.RocketClassifier),\n",
    "#  ('ShapeletTransformClassifier',\n",
    "#   aeon.classification.shapelet_based._stc.ShapeletTransformClassifier),\n",
    "#  ('SignatureClassifier',\n",
    "#   aeon.classification.feature_based._signature_classifier.SignatureClassifier),\n",
    "#  ('SummaryClassifier',\n",
    "#   aeon.classification.feature_based._summary_classifier.SummaryClassifier),\n",
    "#  ('TSFreshClassifier',\n",
    "#   aeon.classification.feature_based._tsfresh_classifier.TSFreshClassifier),\n",
    "#  ('TapNetClassifier',\n",
    "#   aeon.classification.deep_learning.tapnet.TapNetClassifier),\n",
    "#  ('TemporalDictionaryEnsemble',\n",
    "#   aeon.classification.dictionary_based._tde.TemporalDictionaryEnsemble)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471df8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccecac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9388af",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_and_synonyms = [\n",
    "    # Hereditary Hemochromatosis\n",
    "    [\"Hereditary Hemochromatosis\", \"Genetic Iron Overload Disorder\", \"Familial Iron Accumulation Syndrome\", \"Genetic Hemochromatosis\"],\n",
    "    \n",
    "    # Genetic Mutation\n",
    "    [\"Genetic Mutation\", \"Genetic Variant\", \"DNA Alteration\", \"Gene Anomaly\"],\n",
    "    \n",
    "    # Family History\n",
    "    [\"Family History\", \"Ancestral Background\", \"Lineage Record\", \"Inherited Heritage\"],\n",
    "    \n",
    "    # Ethnicity\n",
    "    [\"Ethnicity\", \"Cultural Origin\", \"Ancestral Background\", \"Racial Heritage\"],\n",
    "    \n",
    "    # Gender\n",
    "    [\"Gender\", \"Sex\", \"Biological Gender\", \"Masculine/Feminine Identity\"],\n",
    "    \n",
    "    # Age\n",
    "    [\"Age\", \"Life Stage\", \"Chronological Period\", \"Years Lived\"],\n",
    "    \n",
    "    # Liver Disease\n",
    "    [\"Liver Disease\", \"Hepatic Disorder\", \"Liver Condition\", \"Hepatopathy\"],\n",
    "    \n",
    "    # Alcohol Consumption\n",
    "    [\"Alcohol Consumption\", \"Drinking Behavior\", \"Alcohol Intake\", \"Beverage Consumption\"],\n",
    "    \n",
    "    # Diet\n",
    "    [\"Diet\", \"Nutritional Habits\", \"Eating Regimen\", \"Food Intake\"],\n",
    "    \n",
    "    # Blood Transfusions\n",
    "    [\"Blood Transfusions\", \"Blood Infusions\", \"Transfusion Procedures\", \"Red Blood Cell Transfers\"],\n",
    "    \n",
    "    #indicators_of_hemochromatosis \n",
    "    [\n",
    "    \"Iron overload\",\n",
    "    \"Fatigue\",\n",
    "    \"Joint pain\",\n",
    "    \"Abdominal pain\",\n",
    "    \"Liver enlargement\",\n",
    "    \"Cirrhosis\",\n",
    "    \"Elevated liver enzymes\",\n",
    "    \"Skin pigmentation\",\n",
    "    \"Heart problems\",\n",
    "    \"Diabetes\",\n",
    "    \"Impotence\",\n",
    "    \"Hypothyroidism\",\n",
    "    \"Elevated ferritin levels\",\n",
    "    \"Transferrin saturation\",\n",
    "    \"Genetic mutations (HFE gene)\",\n",
    "    \"MRI to assess iron levels\",\n",
    "    \"Ferretin\",\n",
    "    \"Transferrin\",\n",
    "    \"Iron\",\n",
    "    \n",
    "]\n",
    "]\n",
    "\n",
    "\n",
    "#single var\n",
    "\n",
    "terms_and_synonyms = [\n",
    " [\"Ferretin\",\n",
    "    \"Transferrin\",\n",
    " ]   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298836dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_temp = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms_to_match = [\n",
    "\n",
    "# ]\n",
    "# for elem in terms_and_synonyms:\n",
    "#     for term in elem:\n",
    "#         terms_to_match.append(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a378ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_match = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea8ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in terms_to_match:\n",
    "    if(elem.lower().find(\"ferr\") !=-1):\n",
    "        print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14bcab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbecd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terms_to_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b3c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = {}\n",
    "for term_to_match in tqdm(terms_to_match):\n",
    "    # Fuzzy match against all terms and synonyms\n",
    "    best_match, score = process.extractOne(term_to_match, [item for sublist in terms_and_synonyms for item in sublist])\n",
    "    \n",
    "    # Only consider matches with a certain score threshold (adjust as needed)\n",
    "    if score >= 70:\n",
    "        matches[term_to_match] = best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e11a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_original_terms_set = set(matches.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00528888",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matched_original_terms_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74accbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_original_terms_set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7fc9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_column_filter_list = list(matched_original_terms_set).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_column_filter_list.append('outcome_var_1')\n",
    "manual_column_filter_list.append('client_idcode')\n",
    "manual_column_filter_list.append('date_order_sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5815724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_column_filter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc638fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[manual_column_filter_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7298366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df[manual_column_filter_list].columns:\n",
    "    if df[manual_column_filter_list][column].dtype == 'float':\n",
    "        value_counts = df[manual_column_filter_list][column].value_counts()\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.bar(value_counts.index, value_counts.values)\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Count')\n",
    "        plt.yscale('log')\n",
    "        plt.title(f'Value Counts for {column}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df[manual_column_filter_list].columns:\n",
    "    print(df[manual_column_filter_list][col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[manual_column_filter_list].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c23f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier filter?\n",
    "# # Calculate mean and standard deviation\n",
    "# column_mean = df.mean()\n",
    "# column_std = df.std()\n",
    "\n",
    "# # Define the threshold\n",
    "# threshold = 4 * column_std\n",
    "\n",
    "# # Apply the condition and set values to the mean\n",
    "# for column in df.columns:\n",
    "#     mask = df[column] > (column_mean[column] + threshold[column])\n",
    "#     df.loc[mask, column] = column_mean[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3201e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "col_name_list = ['B12/Folate/Ferritin_num-diagnostic-order', 'Ferritin_days-since-last-diagnostic-order', 'B12/Folate/Ferritin_days-since-last-diagnostic-order',\t'Ferritin_mean', 'Ferritin_num-diagnostic-order']\n",
    "try:\n",
    "    for i in range(0, len(col_name_list)):\n",
    "        biomarker_value = col_name_list[i]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(data=df[manual_column_filter_list], x='date_order_sequence', y=biomarker_value, hue='client_idcode', palette='Set1', legend=False)\n",
    "        plt.title(f'{biomarker_value} Data Visualization')\n",
    "        plt.xlabel('Date Order Sequence')\n",
    "        plt.ylabel(biomarker_value)\n",
    "        #plt.legend(title='Client ID')\n",
    "        plt.show()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ferritin_mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f95956",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_int_name = 'Ferritin_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f93dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for client_id in df['client_idcode'].unique():\n",
    "    client_data = df[df['client_idcode'] == client_id]\n",
    "    plt.plot(client_data['date_order_sequence'], client_data[var_int_name], label=f'Client {client_id} - {var_int_name}' ,linewidth=.1)\n",
    "    #plt.plot(client_data['date'], client_data['variable2'], label=f'Client {client_id} - Variable 2')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title(f'{var_int_name} Over Time')\n",
    "#plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pivot_df = df.pivot(index='client_idcode', columns='date_order_sequence', values=var_int_name)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_df, cmap='YlGnBu')\n",
    "plt.title(f'Time Series Heatmap - {var_int_name}')\n",
    "plt.xlabel('Date Order Sequence')\n",
    "plt.ylabel('Client ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e154a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_filter_columns = False\n",
    "\n",
    "multiprocess = False\n",
    "\n",
    "random_grid_search = True\n",
    "sub_sample_param_space_pct = 0.00000005 # force 2 max\n",
    "\n",
    "grid_n_jobs = 1\n",
    "\n",
    "sub_sample_hyperparameter_search = False\n",
    "\n",
    "sub_sample_hyperparameter_search_val = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(settings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c009c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Generate some random data\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(50)\n",
    "y = np.random.rand(50)\n",
    "\n",
    "# Create the interactive function\n",
    "def update_plot(color):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, c=color, marker='o')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Interactive Scatter Plot')\n",
    "    plt.colorbar(label='Color')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create a dropdown widget\n",
    "color_dropdown = widgets.Dropdown(\n",
    "    options=['red', 'blue', 'green', 'purple'],\n",
    "    value='red',\n",
    "    description='Color:',\n",
    ")\n",
    "\n",
    "# Create the interactive plot using the widget\n",
    "interactive_plot = interact(update_plot, color=color_dropdown)\n",
    "\n",
    "# Display the plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e082794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%prun\n",
    "# Create an empty plot\n",
    "#plt.plot()\n",
    "\n",
    "\n",
    "#for value in settings_list:\n",
    "for i in tqdm(range(0, len(settings_list))):\n",
    "#for i in tqdm(range(0, len(settings_list[0]['outcome_var_n']))):\n",
    "    #i=0\n",
    "    \n",
    "    st = time.time()\n",
    "\n",
    "    value= settings_list[i]\n",
    "\n",
    "    global_param_dict =  value\n",
    "\n",
    "    print(f\"Starting... {global_param_dict}\")\n",
    "\n",
    "    str_b = ''\n",
    "    for key in global_param_dict.keys():\n",
    "        if(key != 'data'):\n",
    "            str_b = str_b + '_' + str(global_param_dict.get(key))\n",
    "        else:\n",
    "            for key in global_param_dict.get('data'):\n",
    "                str_b = str_b + str(int(global_param_dict.get('data').get(key)))\n",
    "\n",
    "    global_param_str = str_b\n",
    "    #global_param_str = str(global_param_dict).replace(\"{\", \"\").replace(\"}\", \"\").replace(\":\", \"\").replace(\" \", \"\").replace(\",\", \"\").replace(\"'\", \"_\").replace(\"__\", \"_\").replace(\"'\",\"\").replace(\",\",\"\").replace(\": \", \"_\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"True\",\"T\").replace(\"False\", \"F\").replace(\" \",\"_\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"_\",\"\")\n",
    "\n",
    "    print(global_param_str)\n",
    "\n",
    "\n",
    "    import sys\n",
    "    import logging\n",
    "    log_folder_path = f\"{global_param_str + additional_naming}/logs/\"\n",
    "    pathlib.Path(base_project_dir+log_folder_path).mkdir(parents=True, exist_ok=True) \n",
    "    nblog = open(f\"{base_project_dir+global_param_str + additional_naming}/logs/log.log\", \"w\")\n",
    "    nblog = open(f\"{base_project_dir+global_param_str + additional_naming}/logs/log.log\", \"a+\")\n",
    "    sys.stdout.echo = nblog\n",
    "    sys.stderr.echo = nblog\n",
    "\n",
    "    get_ipython().log.handlers[0].stream = nblog\n",
    "    get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "    get_ipython().run_line_magic('autosave', '5')\n",
    "\n",
    "\n",
    "    #Set data\n",
    "    def filter_substring_list(string, substr):\n",
    "        return [str for str in string if\n",
    "             any(sub in str for sub in substr) and 'bmi' not in str]\n",
    "    #%%time\n",
    "    #outcome_variable = f'Hemochromatosis (disorder)_count_subject_present_{global_param_dict.get(\"outcome_var_n\")}'\n",
    "\n",
    "    outcome_variable = f'outcome_var_{global_param_dict.get(\"outcome_var_n\")}'\n",
    "\n",
    "\n",
    "    #df_raw = pd.read_csv(input_csv_path)\n",
    "    print(\"Top df shape\", df.shape)\n",
    "    \n",
    "    print(\"Copying data...\")\n",
    "    df_raw = df.copy()\n",
    "    print(\"Copied data...\")\n",
    "\n",
    "    df_imputed = df_raw\n",
    "    \n",
    "    print(\"df_imputed.shape\",df_imputed.shape)\n",
    "    \n",
    "    print(\"Skip dropna\")\n",
    "#     print(\"Set to raw\")\n",
    "\n",
    "#     #     new\n",
    "#     df_raw.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "#     print(\"df_raw.dropna(axis=1, how='all', inplace=True)\")\n",
    "\n",
    "    all_df_columns = list(df_raw.columns)\n",
    "\n",
    "    orignal_feature_names = all_df_columns.copy()\n",
    "\n",
    "    drop_list = []\n",
    "\n",
    "    index_level_list = list(filter(lambda k: '__index_level' in k, all_df_columns))\n",
    "    #print(f\"__index_level: {len(index_level_list)}\")\n",
    "    drop_list.extend(index_level_list)\n",
    "    #df_raw.drop(index_level_list, axis=1, inplace=True)\n",
    "\n",
    "    Unnamed_list = list(filter(lambda k: 'Unnamed' in k, all_df_columns))\n",
    "    #print(f\"Unnamed: {len(index_level_list)}\")\n",
    "    drop_list.extend(Unnamed_list)\n",
    "    #df_raw.drop(Unnamed_list, axis=1, inplace=True)\n",
    "\n",
    "    for elem in all_df_columns:\n",
    "        res = find_near_matches('hfe', elem.lower(), max_l_dist=0)\n",
    "        if(len(res)>0):\n",
    "            #df_raw.drop(elem, axis=1, inplace=True)\n",
    "            drop_list.append(elem)\n",
    "            #print(elem)\n",
    "\n",
    "    for elem in all_df_columns:\n",
    "        res = find_near_matches('client_idcode', elem.lower(), max_l_dist=0)\n",
    "        if(len(res)>0):\n",
    "            #df_raw.drop(elem, axis=1, inplace=True)\n",
    "            drop_list.append(elem)\n",
    "            #print(elem)\n",
    "\n",
    "    for col in df_raw.columns:\n",
    "        if(col.find(\"chrom\") !=-1):\n",
    "            #print(col)\n",
    "            drop_list.append(col)\n",
    "\n",
    "    print(\"------------------------\")\n",
    "\n",
    "\n",
    "    for elem in all_df_columns:\n",
    "        res = find_near_matches(outcome_variable.replace('_count_subject_present_', \"\"), elem.lower(), max_l_dist=0)\n",
    "        if(len(res)>0):\n",
    "            print(elem)\n",
    "\n",
    "\n",
    "    for elem in all_df_columns:\n",
    "        res = find_near_matches('chrom', elem.lower(), max_l_dist=0)\n",
    "        if(len(res)>0):\n",
    "            #f_raw.drop(elem, axis=1, inplace=True)\n",
    "            drop_list.append(elem)\n",
    "            #print(elem)\n",
    "\n",
    "\n",
    "    for elem in all_df_columns:\n",
    "        res = find_near_matches('hfe', elem.lower(), max_l_dist=0)\n",
    "        if(len(res)>0):\n",
    "            #df_raw.drop(elem, axis=1, inplace=True)\n",
    "            drop_list.append(elem)\n",
    "            #print(elem)\n",
    "\n",
    "    blood_test_substrings = ['_mean',\n",
    "                             '_median',\n",
    "                             '_mode',\n",
    "                             '_std',\n",
    "                             '_num-tests',\n",
    "                             '_days-since-last-test',\n",
    "                             '_max',\n",
    "                             '_min',\n",
    "                             '_most-recent',\n",
    "                             '_earliest-test',\n",
    "                             '_days-between-first-last',\n",
    "                             '_contains-extreme-low',\n",
    "                             '_contains-extreme-high']\n",
    "\n",
    "    diagnostic_test_substrings = ['_num-diagnostic-order',\n",
    "                                 '_days-since-last-diagnostic-order',\n",
    "                                 '_days-between-first-last-diagnostic']\n",
    "\n",
    "    drug_order_substrings = ['_num-drug-order',\n",
    "                             '_days-since-last-drug-order',\n",
    "                             '_days-between-first-last-drug']\n",
    "\n",
    "\n",
    "\n",
    "    meta_sp_annotation_count_list = list(filter(lambda k: '_count_subject_present' in k, all_df_columns))\n",
    "    print(f\"_count_subject_present: {len(meta_sp_annotation_count_list)}\")\n",
    "\n",
    "    annotation_count_list = list(filter(lambda k: '_count' in k and '_count_subject_present' not in k, all_df_columns))\n",
    "    print(f\"count_list:             {len(annotation_count_list)}\")\n",
    "    \n",
    "    not_meta_sp_annotation_count_list = list(\n",
    "        filter(lambda k: '_count_subject_not_present' in k, all_df_columns))\n",
    "\n",
    "    \n",
    "    meta_rp_annotation_count_list = list(\n",
    "        filter(lambda k: '_count_relative_present' in k, all_df_columns))\n",
    "    \n",
    "    not_meta_rp_annotation_count_list = list(\n",
    "        filter(lambda k: '_count_relative_not_present' in k, all_df_columns))\n",
    "    \n",
    "    \n",
    "    meta_sp_annotation_count_list.extend(not_meta_sp_annotation_count_list)\n",
    "    \n",
    "    meta_sp_annotation_count_list.extend(meta_rp_annotation_count_list)\n",
    "        \n",
    "    meta_sp_annotation_count_list.extend(not_meta_rp_annotation_count_list)\n",
    "\n",
    "    diagnostic_order_list =[]# list(filter(lambda k: '-diagnostic-order' in k, all_df_columns))\n",
    "    diagnostic_list = filter_substring_list(all_df_columns, diagnostic_test_substrings)\n",
    "    diagnostic_order_list.extend(diagnostic_list)\n",
    "    print(f\"diagnostic_order_list:  {len(diagnostic_order_list)}\")\n",
    "\n",
    "    drug_order_list = []#list(filter(lambda k: '-drug-order' in k, all_df_columns))\n",
    "    drug_list = filter_substring_list(all_df_columns, drug_order_substrings)\n",
    "    drug_order_list.extend(drug_list)\n",
    "    print(f\"drug_order_list:        {len(drug_order_list)}\")\n",
    "\n",
    "    bmi_list = list(filter(lambda k: 'bmi_' in k, all_df_columns))\n",
    "    print(f\"bmi_list:        {len(bmi_list)}\")\n",
    "\n",
    "    ethnicity_list = list(filter(lambda k: 'census_' in k, all_df_columns))\n",
    "    print(f\"ethnicity_list:        {len(ethnicity_list)}\")\n",
    "\n",
    "    annotation_mrc_count_list = list(filter(lambda k: '_count_mrc_cs' in k, all_df_columns))\n",
    "    print(f\"_count_mrc_cs:        {len(annotation_mrc_count_list)}\")\n",
    "\n",
    "    meta_sp_annotation_mrc_count_list = list(filter(lambda k: '_count_subject_present_mrc_cs' in k, all_df_columns))\n",
    "    print(f\"_count_subject_present_mrc_cs_list:        {len(meta_sp_annotation_mrc_count_list)}\")\n",
    "    \n",
    "    not_meta_sp_annotation_mrc_count_list = list(\n",
    "        filter(lambda k: '_count_subject_not_present_mrc_cs' in k, all_df_columns))\n",
    "    \n",
    "    \n",
    "    relative_meta_rp_annotation_mrc_count_list = list(\n",
    "        filter(lambda k: '_count_relative_present_mrc_cs' in k, all_df_columns))\n",
    "    \n",
    "    not_relative_meta_rp_annotation_mrc_count_list = list(\n",
    "        filter(lambda k: '_count_relative_not_present_mrc_cs' in k, all_df_columns))   \n",
    "    \n",
    "    \n",
    "    meta_sp_annotation_mrc_count_list.extend(not_meta_sp_annotation_mrc_count_list)\n",
    "    \n",
    "    meta_sp_annotation_mrc_count_list.extend(relative_meta_rp_annotation_mrc_count_list)\n",
    "    \n",
    "    meta_sp_annotation_mrc_count_list.extend(not_relative_meta_rp_annotation_mrc_count_list)\n",
    "\n",
    "    core_02_list = list(filter(lambda k: 'core_02_' in k, all_df_columns))\n",
    "    print(f\"core_02_list:        {len(core_02_list)}\")\n",
    "\n",
    "    bed_list = list(filter(lambda k: 'bed_' in k, all_df_columns))\n",
    "    print(f\"bed_list:        {len(bed_list)}\")\n",
    "\n",
    "    vte_status_list = list(filter(lambda k: 'vte_status_' in k, all_df_columns))\n",
    "    print(f\"vte_status_list:        {len(vte_status_list)}\")\n",
    "\n",
    "    hosp_site_list = list(filter(lambda k: 'hosp_site_' in k, all_df_columns))\n",
    "    print(f\"hosp_site_list:        {len(hosp_site_list)}\")\n",
    "\n",
    "    core_resus_list = list(filter(lambda k: 'core_resus_' in k, all_df_columns))\n",
    "    print(f\"core_resus_list:        {len(core_resus_list)}\")\n",
    "\n",
    "    news_list = list(filter(lambda k: 'news_resus_' in k, all_df_columns))\n",
    "    print(f\"news_list:        {len(news_list)}\")\n",
    "\n",
    "    date_time_stamp_list = list(filter(lambda k: '_date_time_stamp' in k, all_df_columns))\n",
    "    print(f\"date_time_stamp_list:        {len(date_time_stamp_list)}\")\n",
    "\n",
    "\n",
    "    bloods_list = filter_substring_list(all_df_columns, blood_test_substrings)\n",
    "    print(f\"bloods_list: {len(bloods_list)}\")\n",
    "\n",
    "    candidate_feature_category_lists=[meta_sp_annotation_count_list, annotation_count_list, diagnostic_order_list, drug_order_list, bmi_list, ethnicity_list, bloods_list]\n",
    "\n",
    "    pertubation_columns = []\n",
    "\n",
    "    if(value.get('data').get('age') == True):\n",
    "        pertubation_columns.append('age')\n",
    "\n",
    "    if(value.get('data').get('sex') == True):\n",
    "        pertubation_columns.append('male')\n",
    "\n",
    "    if(value.get('data').get('bmi') == True):\n",
    "        pertubation_columns.extend(bmi_list)\n",
    "\n",
    "    if(value.get('data').get('ethnicity') == True):\n",
    "        pertubation_columns.extend(ethnicity_list)\n",
    "\n",
    "    if(value.get('data').get('bloods') == True):\n",
    "        pertubation_columns.extend(bloods_list)\n",
    "\n",
    "    if(value.get('data').get('diagnostic_order') == True):\n",
    "        pertubation_columns.extend(diagnostic_order_list)\n",
    "\n",
    "    if(value.get('data').get('drug_order') == True):\n",
    "        pertubation_columns.extend(drug_order_list)\n",
    "\n",
    "    if(value.get('data').get('annotation_n') == True):\n",
    "        pertubation_columns.extend(annotation_count_list)\n",
    "\n",
    "    if(value.get('data').get('meta_sp_annotation_n') == True):\n",
    "        pertubation_columns.extend(meta_sp_annotation_count_list)\n",
    "\n",
    "    if(value.get('data').get('annotation_mrc_n') == True):\n",
    "        pertubation_columns.extend(annotation_mrc_count_list)\n",
    "\n",
    "    if(value.get('data').get('meta_sp_annotation_mrc_n') == True):\n",
    "        pertubation_columns.extend(meta_sp_annotation_mrc_count_list)\n",
    "\n",
    "    if(value.get('data').get('core_02') == True):\n",
    "        pertubation_columns.extend(core_02_list)\n",
    "\n",
    "    if(value.get('data').get('bed') == True):\n",
    "        pertubation_columns.extend(bed_list)\n",
    "\n",
    "    if(value.get('data').get('vte_status') == True):\n",
    "        pertubation_columns.extend(vte_status_list)\n",
    "\n",
    "    if(value.get('data').get('hosp_site') == True):\n",
    "        pertubation_columns.extend(hosp_site_list)\n",
    "\n",
    "    if(value.get('data').get('core_resus') == True):\n",
    "        pertubation_columns.extend(core_resus_list)\n",
    "\n",
    "    if(value.get('data').get('news') == True):\n",
    "        pertubation_columns.extend(news_list)\n",
    "\n",
    "    if(value.get('data').get('date_time_stamp') == True):\n",
    "        pertubation_columns.extend(date_time_stamp_list)    \n",
    "\n",
    "\n",
    "    display(global_param_dict.get('data'))\n",
    "\n",
    "    print(f\"Using {len(pertubation_columns)}/{len(all_df_columns)} columns\")\n",
    "\n",
    "    list_2 = df_raw.columns\n",
    "    list_1 = pertubation_columns.copy()\n",
    "\n",
    "    difference_list = list(set(list_2) - set(list_1))\n",
    "    print(f\"Omitting {len(difference_list)} :...\")\n",
    "    print(f\"{difference_list[0:5]}...\")\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    imputed_cols = list(df_raw.columns)\n",
    "    #     scaler = StandardScaler()\n",
    "\n",
    "    #     scaler.fit(df_raw)\n",
    "\n",
    "    #     scaled_imputed = scaler.transform(df_raw)\n",
    "\n",
    "    #     scaled_imputed_df = pd.DataFrame(data=scaled_imputed, columns = imputed_cols)\n",
    "\n",
    "    #temp_col_list = list(df_raw.columns)\n",
    "    print(\"col d type handle\")\n",
    "    #print(\"temp_col_list = list(df_raw.select_dtypes(include=[float, int]).columns)\")\n",
    "    temp_col_list = list(df_raw.columns)\n",
    "    temp_col_list.remove(\"client_idcode\")\n",
    "    #temp_col_list = list(df_raw.select_dtypes(include=[float, int]).columns)\n",
    "\n",
    "    #corr_matrix = pd.DataFrame(np.corrcoef(df_raw.values, rowvar=False), columns=temp_col_list).abs()\n",
    "    print(\"corr_matrix\")\n",
    "    #corr_matrix = pd.DataFrame(np.corrcoef(df_raw.drop('client_idcode', axis=1).values, rowvar=False), columns=temp_col_list).abs()\n",
    "    temp_col_list.remove('date')\n",
    "    corr_matrix = pd.DataFrame(np.corrcoef(df_raw.drop('client_idcode', axis=1).drop('date', axis=1).values, rowvar=False), columns=temp_col_list).abs()\n",
    "\n",
    "    # Create a True/False mask and apply it\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    tri_df = corr_matrix.mask(mask)\n",
    "    # List column names of highly correlated features (r >0.5 )\n",
    "    corr_to_drop = [c for c in tri_df.columns if any(tri_df[c] > value.get('corr'))]\n",
    "\n",
    "    #reduced_df = scaled_imputed_df.drop(corr_to_drop, axis=1)\n",
    "    print(f\"Identified {len(corr_to_drop)} correlated features to drop at >{value.get('corr')}\")\n",
    "    drop_list.extend(corr_to_drop)\n",
    "\n",
    "    print(\"------------------------\")\n",
    "    percent_missing_dict = pd.read_csv('missing_combined_10kdf')\n",
    "    percent_missing_dict = percent_missing_dict[['column_name','percent_missing']]\n",
    "\n",
    "\n",
    "    percent_missing_threshold = value.get('percent_missing')\n",
    "    percent_missing_drop_list = []\n",
    "    for col in all_df_columns:\n",
    "        try:\n",
    "            if(percent_missing_dict[percent_missing_dict['column_name']==col]['percent_missing'].values[0]>percent_missing_threshold):\n",
    "                percent_missing_drop_list.append(col)\n",
    "        except:\n",
    "            pass\n",
    "    print(f\"Identified {len(percent_missing_drop_list)} at > {percent_missing_threshold} threshold\")\n",
    "    drop_list.extend(percent_missing_drop_list)\n",
    "\n",
    "\n",
    "    print(f\"Drop_list: \")\n",
    "    print('\\n'.join(map(str, drop_list[0:5])))\n",
    "\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    \n",
    "    print(\"Extending all outcome list on drop list\")\n",
    "    drop_list.extend(all_outcome_list)\n",
    "\n",
    "    try:\n",
    "        drop_list.remove(outcome_variable)\n",
    "    except: \n",
    "        print(outcome_variable + \"not in drop list\")\n",
    "\n",
    "    #df_raw.drop(drop_list, axis=1, inplace=True)\n",
    "\n",
    "    #df_raw.drop(['datetime', 'datetime.1'], axis=1, inplace=True)\n",
    "\n",
    "    #handle sequence columns: ? \n",
    "    # cols = ['client_idcode', 'date_order_sequence'] + [col for col in df.columns if col not in ['client_idcode', 'date_order_sequence']]\n",
    "    # df = df[cols]\n",
    "\n",
    "    # cols = [col for col in df.columns if col != 'outcome_var_1'] + ['outcome_var_1']\n",
    "    # df = df[cols]\n",
    "\n",
    "\n",
    "\n",
    "    final_column_list = [x for x in pertubation_columns if (x not in drop_list)]\n",
    "\n",
    "    final_column_list = final_column_list#[0:50] # temporary fix\n",
    "\n",
    "    final_column_list.extend(['client_idcode', 'date_order_sequence', 'outcome_var_1'])\n",
    "    print(\"manually_filter_columns ...\", manually_filter_columns)\n",
    "    \n",
    "    if(manually_filter_columns):\n",
    "        df_imputed = df_raw[manual_column_filter_list].copy()\n",
    "    else:\n",
    "    \n",
    "        df_imputed = df_raw[final_column_list].copy()\n",
    "\n",
    "    print(\"dropping duplicated columns\")\n",
    "\n",
    "    df_imputed = df_imputed.loc[:,~df_imputed.columns.duplicated()].copy()\n",
    "    \n",
    "    print(\"Fragment fix\")\n",
    "    df_imputed = df_imputed.copy()\n",
    "\n",
    "    print(\"Screening for non float data types:\")\n",
    "    types = []\n",
    "    for col in df_imputed.columns:\n",
    "        if(df_imputed[col].dtype != int and df_imputed[col].dtype != float):\n",
    "            print(col)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # y = df_raw[outcome_variable]\n",
    "\n",
    "    # X = df_imputed\n",
    "\n",
    "    \n",
    "\n",
    "    unique_pats = df_imputed[['client_idcode', 'date_order_sequence']]['client_idcode'].unique()\n",
    "\n",
    "    if(sub_sample_columns and sub_sample_columns_init ==False):\n",
    "        feature_list = list(df_imputed.columns)\n",
    "        \n",
    "        feature_list = list(pd.Series(feature_list).sample(col_sample_n)) #implemented at top, toggle with _init\n",
    "        feature_list = set(feature_list)\n",
    "        feature_list.add('outcome_var_1')\n",
    "        feature_list.add('male')\n",
    "        feature_list.add('age')\n",
    "        \n",
    "        feature_list = list(feature_list)\n",
    "    else:\n",
    "        \n",
    "        #feature_list = df_imputed.columns #mistkae?\n",
    "        feature_list = final_column_list\n",
    "\n",
    "        \n",
    "    seed_n = random.randint(0, 9999)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #enumerate to get split with fixed seed for feature method. dataframe then array\n",
    "    \n",
    "    X = df_imputed[list(set(feature_list))]\n",
    "    y = df_imputed['outcome_var_1']\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=seed_n\n",
    "        )\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_orig, y_train_orig, test_size=0.25, random_state=seed_n\n",
    "        )   \n",
    "    \n",
    "    target_n_features = global_param_dict.get('n_features')\n",
    "    if(target_n_features != 100):\n",
    "        \n",
    "        \n",
    "        target_n_features_eval = int((target_n_features/100) * len(X_train.columns))\n",
    "        \n",
    "            \n",
    "        if(target_n_features_eval < len(X_train.columns)):\n",
    "            target_n_features_eval = len(X_train.columns)\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    print(f\"Pre target_n_features {target_n_features}% reduction {target_n_features_eval}/{len(X_train.columns)}\")\n",
    "\n",
    "    print(f\"target_n_features: {target_n_features}\")\n",
    "    \n",
    "    #set this func to use X not X_train for post filtered col names\n",
    "    features = getNfeaturesANOVAF(target_n_features_eval)\n",
    "    print(\"pre droplist on features len(features)\", len(features))\n",
    "    features = [x for x in features if (x not in drop_list)]\n",
    "    \n",
    "    \n",
    "    print(\" post len(features)\", len(features))    \n",
    "    \n",
    "    features = [x for x in features if (x not in final_column_list)]\n",
    "    print(\" post features if (x not in final_column_list)\", len(features))  \n",
    "    feature_list = features\n",
    "        \n",
    "    X = []\n",
    "    y = []    \n",
    "        \n",
    "    print(\"Feature_list\")\n",
    "    print(feature_list)\n",
    "    print(f\"len(feature_list) :{len(feature_list) }\")\n",
    "    #print(feature_list)\n",
    "    for i in tqdm(range(0, len(unique_pats))): #verbose #unique_pats\n",
    "        \n",
    "        pat = unique_pats[i]\n",
    "\n",
    "        y.append(df_imputed[df_imputed['client_idcode']==pat].iloc[0]['outcome_var_1']) \n",
    "\n",
    "        pat_multi_vector = sequence.pad_sequences(np.transpose(df_imputed[df_imputed['client_idcode']==pat][feature_list].values), maxlen=max_seq_length)\n",
    "        X.append(pat_multi_vector)\n",
    "\n",
    "\n",
    "\n",
    "    rename_cols = True\n",
    "    if(rename_cols):\n",
    "        regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "        df_imputed.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in df_imputed.columns.values]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    file_path = base_project_dir + global_param_str + additional_naming\n",
    "    plot_path = base_project_dir + global_param_str + additional_naming + '/figures/' #file_path\n",
    "    Path(plot_path).mkdir(parents=True, exist_ok=True)\n",
    "    #Path(plot_path + \"_scaled//\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    init_live_scores = []\n",
    "    \n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    global_param_dict['resample'] = None\n",
    "    \n",
    "    \n",
    "    if(global_param_dict.get('resample') ==None):\n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=1\n",
    "        )\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_orig, y_train_orig, test_size=0.25, random_state=1\n",
    "        )   \n",
    "\n",
    "\n",
    "    elif(global_param_dict.get('resample') =='undersample'):\n",
    "        \n",
    "        rus = RandomUnderSampler(random_state=0)\n",
    "        X, y = rus.fit_resample(X, y)\n",
    "        #Create validation set\n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=1\n",
    "        )\n",
    "\n",
    "        #X = X_train_orig.copy()\n",
    "        #y = y_train_orig.copy()\n",
    "        #Resplit holding back _orig\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_orig, y_train_orig, test_size=0.25, random_state=1\n",
    "        )\n",
    "        X = X_train_orig.copy()\n",
    "        y = y_train_orig.copy()\n",
    "\n",
    "    elif(global_param_dict.get('resample') =='oversample'):\n",
    "        #Train test split then oversample to avoid poision\n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=1\n",
    "        )\n",
    "        \n",
    "        sampling_strategy = 0.8\n",
    "        ros = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "        X_res, y_res = ros.fit_resample(X_train_orig, y_train_orig)\n",
    "        print(y_res.value_counts())\n",
    "        X = X_res.copy()\n",
    "        y = y_res.copy()\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     features = getNfeaturesANOVAF(target_n_features_eval)\n",
    "\n",
    "#     X_train = X_train[features]\n",
    "#     X_train_orig = X_train_orig[features]\n",
    "#     X_test = X_test[features]\n",
    "#     X_train_orig = X_train_orig[features]\n",
    "\n",
    "    display(pd.Series(y).value_counts())\n",
    "\n",
    "    pd.Series(y).plot(kind=\"hist\")\n",
    "\n",
    "    scale = global_param_dict.get('scale')\n",
    "\n",
    "        #plot_path = plot_path + \"_scaled//\"\n",
    "\n",
    "    if(global_param_dict.get('param_space_size') == 'medium'):\n",
    "        log_small = np.logspace(-1, -5, 3)\n",
    "        print(log_small)\n",
    "        bool_param = [True, False]\n",
    "        print(bool_param)\n",
    "        log_large = np.logspace(0, 2, 3).astype(int)\n",
    "        print(log_large)\n",
    "        log_large_long = np.floor(np.logspace(0, 2.8, 6)).astype(int)\n",
    "        print(log_large_long)\n",
    "        log_med_long = np.floor(np.logspace(0, 1.5, 5)).astype(int)\n",
    "        print(log_med_long)\n",
    "        log_med = np.floor(np.logspace(0, 1.5, 3)).astype(int)\n",
    "        print(log_med)\n",
    "        nstep = 3\n",
    "        log_zero_one = np.logspace(0.0, 1.0, nstep) / 10\n",
    "        print(log_zero_one)\n",
    "        lin_zero_one = np.linspace(0.0, 1.0, nstep) / 10\n",
    "        print(lin_zero_one)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    model_class_list = [\n",
    "          \n",
    "        FCNClassifier_class, #working l \n",
    "        MLPClassifier_class, #working l \n",
    "        TapNetClassifier_class, #working l \n",
    "        Arsenal_class, #working l \n",
    "        IndividualTDE_class, #working l\n",
    "        DrCIF_class, # working l\n",
    "        \n",
    "\n",
    "        MUSE_class, #working L\n",
    "        TSFreshClassifier_class, #working L\n",
    "        IndividualInceptionClassifier_class, #working L\n",
    "        CNNClassifier_class, #can work L\n",
    "        RocketClassifier_class, #working L\n",
    "        \n",
    "        InceptionTimeClassifier_class, #can work L         \n",
    "        EncoderClassifier_class, #can work L\n",
    "        FreshPRINCEClassifier_class, #can work L\n",
    "        KNeighborsTimeSeriesClassifier_class, #working #vslow on njob 1 L\n",
    "       ElasticEnsemble_class,#possibly univariate non working? param problems\n",
    "       OrdinalTDE_class, #working prob but on small datA?\n",
    "       HIVECOTEV2_class, #slow or broken?\n",
    "       TemporalDictionaryEnsemble_class, #problem\n",
    "       Catch22Classifier_class, # can work\n",
    "       SummaryClassifier_class,\n",
    "        \n",
    "        \n",
    "        #ResNetClassifier_class, #not working unknown param problem need example\n",
    "        #SignatureClassifier_class, # problem, troubleshoot\n",
    "        #CanonicalIntervalForest_class,# Nnot found\n",
    "        \n",
    "        \n",
    "        #ShapeDTW_class, #uni variate\n",
    "        #HIVECOTEV1_class, #univariate\n",
    "        #ContractableBOSS_class, #univariate\n",
    "        \n",
    "    ]\n",
    "    #random.shuffle(model_class_list)\n",
    "\n",
    "    for elem in model_class_list:\n",
    "        pg = ParameterGrid(elem.parameter_space)\n",
    "        print(elem.method_name)\n",
    "        print(len(pg))\n",
    "        for param in elem.parameter_space:\n",
    "\n",
    "            if(isinstance(elem.parameter_space.get(param), list) is False and isinstance(elem.parameter_space.get(param), np.ndarray) is False):\n",
    "                print(elem.method_name, param)\n",
    "                print(type(elem.parameter_space.get(param)))\n",
    "\n",
    "    scores_tuple_list = []\n",
    "    model_error_list = []\n",
    "    \n",
    "    pg_list = []\n",
    "    for elem in model_class_list:\n",
    "\n",
    "        pg = ParameterGrid(elem.parameter_space)\n",
    "\n",
    "        pg_list.append(len(ParameterGrid(elem.parameter_space)))\n",
    "\n",
    "    mean_parameter_space_val = np.mean(pg_list)\n",
    "                                \n",
    "    sub_sample_parameter_val = int(sub_sample_param_space_pct * mean_parameter_space_val) \n",
    "    \n",
    "    sub_sample_parameter_val = sub_sample_parameter_val + 2\n",
    "    \n",
    "    #X_train, y_train, X_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict\n",
    "    \n",
    "    arg_list = []\n",
    "    for model_class in model_class_list:\n",
    "\n",
    "        class_name = model_class\n",
    "        \n",
    "        #grid_search_cv(X_train, Y_train, x_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict):\n",
    "    \n",
    "\n",
    "        arg_list.append(\n",
    "            (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test, \n",
    "                class_name.algorithm_implementation,\n",
    "                class_name.parameter_space,\n",
    "                class_name.method_name,\n",
    "                X,\n",
    "                y,\n",
    "                sub_sample_parameter_val\n",
    "                #class_name.parameter_space\n",
    "                \n",
    "            )\n",
    "        )\n",
    "\n",
    "    #input()\n",
    "    # multiprocess = multiprocessing_bool #warning enabling will break classifier grid scores pickle, also single needs patch\n",
    "\n",
    "\n",
    "    # if multiprocess == True:\n",
    "\n",
    "    #     def multi_run_wrapper(args):\n",
    "    #         return grid_search_crossvalidate(*args)\n",
    "\n",
    "    #     if __name__ == \"__main__\":\n",
    "    #         from multiprocessing import Pool\n",
    "\n",
    "    #         pool = Pool(8)\n",
    "    #         results = pool.map(multi_run_wrapper, arg_list)\n",
    "    #         # print(results)\n",
    "    #         pool.close() # exp\n",
    "\n",
    "    print(f\"Main on {len(all_df_columns)-len(drop_list)} features X_train: {X_train.shape}\")\n",
    "    \n",
    "    if multiprocess == False:\n",
    "        for k in range(0, len(arg_list)):\n",
    "            attempt_n = 1\n",
    "            model_param_pass = False\n",
    "            while model_param_pass ==False and attempt_n < 3:\n",
    "    \n",
    "                try:\n",
    "                    print(\"Running: \", arg_list[k][6])\n",
    "                    grid_search_cv(\n",
    "                        *arg_list[k]\n",
    "                    )\n",
    "                    attempt_n = 0\n",
    "                    model_param_pass = True\n",
    "                    print(f\"set model_param_pass True for {arg_list[k][6]}\")\n",
    "                    #break\n",
    "#                 except CudaAPIError as e:\n",
    "#                     print(\"CudaAPIError on \", arg_list[k][6])\n",
    "#                     print(e)\n",
    "#                     model_error_list.append((arg_list[k][4], e)) \n",
    "#                     input()\n",
    "#                     #break\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    print(\"ValueError on \", arg_list[k][6])\n",
    "                    print(e)\n",
    "                    model_error_list.append((arg_list[k][4], e)) \n",
    "                    print(f\"Attempt no {attempt_n}\")\n",
    "#                     grid_search_cv(\n",
    "#                         *arg_list[k]\n",
    "#                     )\n",
    "            \n",
    "                except Exception as e:\n",
    "                    print(traceback.format_exc())\n",
    "                    print(\"error on \", arg_list[k][6])\n",
    "                    model_error_list.append((arg_list[k][4], e)) \n",
    "                    input()\n",
    "                \n",
    "                attempt_n = attempt_n + 1\n",
    "        \n",
    "\n",
    "    print(model_error_list)\n",
    "\n",
    "\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06596a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_n_features_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cb9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35da05",
   "metadata": {},
   "source": [
    "# test bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e235247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824221f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Recommendation to (procedure)_count_relative_not_present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = 'Ferritin_mean'\n",
    "sklearn.feature_selection.f_classif(\n",
    "                           np.array(X_train[colName]).reshape(-1, 1), y_train\n",
    "                      )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b1c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "getNfeaturesANOVAF(target_n_features_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f213e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d85c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "getNfeaturesANOVAF_spec( 4, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79896517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da346b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample_parameter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cuddnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Driver Version: 495.29.05    CUDA Version: 11.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f0deb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8368fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall tensorflow-probability -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42549e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78856cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in model_class_list:\n",
    "    pg = ParameterGrid(elem.parameter_space)\n",
    "    print(elem.method_name)\n",
    "    print(len(pg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7d6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U aeon[all_extras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bdff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fcf481",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed[df_imputed['client_idcode']==pat][feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed[df_imputed['client_idcode']==pat][feature_list].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(df_imputed[df_imputed['client_idcode']==pat][feature_list].values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03424b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence.pad_sequences(np.transpose(df_imputed[df_imputed['client_idcode']==pat][feature_list].values), maxlen=max_seq_length).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db2d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_multi_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(unique_pats))): #verbose #unique_pats\n",
    "    #for i in (range(0, len(unique_pats))):\n",
    "    pat = unique_pats[i]\n",
    "\n",
    "    y.append(df_imputed[df_imputed['client_idcode']==pat].iloc[0]['outcome_var_1']) \n",
    "\n",
    "    pat_multi_vector = sequence.pad_sequences(np.transpose(df_imputed[df_imputed['client_idcode']==pat][feature_list].values), maxlen=max_seq_length)\n",
    "    X.append(pat_multi_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len('uu'))): #verbose #unique_pats\n",
    "    #for i in (range(0, len(unique_pats))):\n",
    "    pat = unique_pats[i]\n",
    "\n",
    "    y.append(df_imputed[df_imputed['client_idcode']==pat].iloc[0]['outcome_var_1']) \n",
    "\n",
    "    pat_multi_vector = []\n",
    "\n",
    "    for j in range(0, len(feature_list)):\n",
    "\n",
    "        pat_col_vals_raw = df_imputed[df_imputed['client_idcode']==pat][feature_list[j]].to_list()\n",
    "\n",
    "        pat_multi_vector.append(pat_col_vals_raw)\n",
    "\n",
    "    pat_multi_vector = sequence.pad_sequences(pat_multi_vector, maxlen=max_seq_length)\n",
    "    X.append(pat_multi_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46c7e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def process_pat(i, unique_pats, df_imputed, y, feature_list, X, max_seq_length):\n",
    "    pat = unique_pats[i]\n",
    "    y.append(df_imputed[df_imputed['client_idcode'] == pat].iloc[0]['outcome_var_1'])\n",
    "\n",
    "    pat_multi_vector = []\n",
    "    for j in range(0, len(feature_list)):\n",
    "        pat_col_vals_raw = df_imputed[df_imputed['client_idcode'] == pat][feature_list[j]].to_list()\n",
    "        pat_multi_vector.append(pat_col_vals_raw)\n",
    "\n",
    "    pat_multi_vector = sequence.pad_sequences(pat_multi_vector, maxlen=max_seq_length)\n",
    "    X.append(pat_multi_vector)\n",
    "\n",
    "def main():\n",
    "    # Assuming you have initialized these variables\n",
    "    unique_pats = ...\n",
    "    df_imputed = ...\n",
    "    y = []\n",
    "    feature_list = ...\n",
    "    X = []\n",
    "    max_seq_length = ...\n",
    "\n",
    "    num_processes = multiprocessing.cpu_count()  # Use the number of available CPU cores\n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "\n",
    "    func = partial(\n",
    "        process_pat,\n",
    "        unique_pats=unique_pats,\n",
    "        df_imputed=df_imputed,\n",
    "        y=y,\n",
    "        feature_list=feature_list,\n",
    "        X=X,\n",
    "        max_seq_length=max_seq_length\n",
    "    )\n",
    "\n",
    "    for _ in tqdm(pool.imap_unordered(func, range(len(unique_pats))), total=len(unique_pats)):\n",
    "        pass\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae0545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb598902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualInceptionClassifier_class():\n",
    "    algorithm_implementation = IndividualInceptionClassifier\n",
    "\n",
    "    method_name = 'IndividualInceptionClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'depth': [6, 8, 10],\n",
    "        'nb_filters': [32, 64, 128],\n",
    "        'nb_conv_per_layer': [3, 4, 5],\n",
    "        'kernel_size': [30, 40, 50],\n",
    "        'use_max_pooling': [True, False],\n",
    "        'max_pool_size': [2, 3, 4],\n",
    "        'strides': [1, 2],\n",
    "        'dilation_rate': [1, 2],\n",
    "        'padding': ['same', 'valid'],\n",
    "        'activation': ['relu', 'elu'],\n",
    "        'use_bias': [True, False],\n",
    "        'use_residual': [True, False],\n",
    "        'use_bottleneck': [True, False],\n",
    "        'bottleneck_size': [16, 32, 64],\n",
    "        'use_custom_filters': [True, False],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'use_mini_batch_size': [True, False],\n",
    "        'n_epochs': [1000, 1500, 2000],\n",
    "        #'callbacks': [None, [ReduceOnPlateau(), ModelCheckpoint()]],\n",
    "        #'file_path': ['./', '/path/to/save'],\n",
    "        #'save_best_model': [True, False],\n",
    "        #'save_last_model': [True, False],\n",
    "        #'best_file_name': ['best_model', 'model_best'],\n",
    "        #'last_file_name': ['last_model', 'model_last'],\n",
    "        'random_state': [random_state_val],\n",
    "        'verbose': [True],\n",
    "        #'optimizer': [Adam(), RMSprop(), SGD()],\n",
    "        'loss': ['categorical_crossentropy', 'binary_crossentropy'],\n",
    "        'metrics': ['accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNClassifier_class():\n",
    "    algorithm_implementation = FCNClassifier\n",
    "    \n",
    "    method_name = 'FCNClassifier'\n",
    "    \n",
    "    parameter_space = {\n",
    "        'n_layers': [3],\n",
    "#         'n_filters': [128, 256, 128],\n",
    "#         'kernel_size': [8, 5, 3],\n",
    "#         'dilation_rate': [1],\n",
    "#         'strides': [1],\n",
    "#         'padding': ['same'],\n",
    "#         'activation': ['relu'],\n",
    "#         'use_bias': [True],\n",
    "#         'n_epochs': [2000, 1000],\n",
    "#         'batch_size': [16],\n",
    "#         'use_mini_batch_size': [True],\n",
    "#         'random_state': [None],\n",
    "#         'verbose': [True],\n",
    "#         'loss': ['categorical_crossentropy'],\n",
    "#         'metrics': [None],\n",
    "#         'optimizer': [None],\n",
    "        \n",
    "        #'file_path': ['./'],\n",
    "        #'save_best_model': [False],\n",
    "        #'save_last_model': [False],\n",
    "        #'best_file_name': ['best_model'],\n",
    "        #'last_file_name': ['last_model'],\n",
    "        #'callbacks': [None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4514c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10014021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVECOTEV2_class():\n",
    "    algorithm_implementation = HIVECOTEV2\n",
    "\n",
    "    method_name = 'HIVECOTEV2'\n",
    "\n",
    "    parameter_space = {\n",
    "        'stc_params': [None, ],  # Parameters for the ShapeletTransformClassifier module. If None, uses the default parameters with a 2-hour transform contract.\n",
    "        'drcif_params': [None, ],  # Parameters for the DrCIF module. If None, uses the default parameters with n_estimators set to 500.\n",
    "        'arsenal_params': [None, ],  # Parameters for the Arsenal module. If None, uses the default parameters.\n",
    "        'tde_params': [None, ],  # Parameters for the TemporalDictionaryEnsemble module. If None, uses the default parameters.\n",
    "        'time_limit_in_minutes': time_limit_param,  # Time contract to limit build time in minutes, overriding n_estimators/n_parameter_samples for each component. Default of 0 means n_estimators/n_parameter_samples for each component is used.\n",
    "        'save_component_probas': [False],  # When predict/predict_proba is called, save each HIVE-COTEV2 component probability predictions in component_probas.\n",
    "        'verbose': [1],  # Level of output printed to the console (for information only).\n",
    "        'random_state': [random_state_val],  # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "        'n_jobs': [-1],  # The number of jobs to run in parallel for both fit and predict. -1 means using all processors.\n",
    "        #'parallel_backend': [ 'multiprocessing'], #None, 'loky',  , 'threading'# Specify the parallelization backend implementation in joblib for Catch22, if None a ‘prefer’ value of “threads” is used by default. Valid options are “loky”, “multiprocessing”, “threading” or a custom backend. See the joblib Parallel documentation for more details.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLPClassifier_class():\n",
    "    algorithm_implementation = MLPClassifier\n",
    "\n",
    "    method_name = 'MLPClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_epochs': [1000, 1500, 2000],                         # Number of epochs to train the model\n",
    "        'batch_size': [8, 16, 32],                              # Number of samples per gradient update\n",
    "        'random_state': [random_state_val],                                # Seed for random number generation\n",
    "        'verbose': [True, False],                               # Whether to output extra information\n",
    "        'loss': ['binary_crossentropy'],  # Fit parameter for the Keras model #must be binary? #'mean_squared_error', \n",
    "        #'file_path': ['./', '/models'],                         # File path when saving ModelCheckpoint callback\n",
    "        #'save_best_model': [True, False],                       # Whether or not to save the best model\n",
    "        #'save_last_model': [True, False],                       # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'top_model'],          # The name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'final_model'],        # The name of the file of the last model\n",
    "        'optimizer': [keras.optimizers.Adadelta(), keras.optimizers.Adam()],  # Keras optimizer\n",
    "        'metrics': [['accuracy'], ['accuracy', 'mae']],         # List of strings for metrics\n",
    "        'activation': ['sigmoid', 'relu'],                      # Activation function used in the output linear layer\n",
    "        'use_bias': [True, False],                              # Whether the layer uses a bias vector\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedba7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TapNetClassifier_class():\n",
    "    algorithm_implementation = TapNetClassifier\n",
    "\n",
    "    method_name = 'TapNetClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'filter_sizes': [(256, 256, 128), (128, 128, 64)],  # Sets the kernel size argument for each convolutional block. Controls the number of convolutional filters and the number of neurons in attention dense layers.\n",
    "        'kernel_size': [(8, 5, 3), (4, 3, 2)],  # Controls the size of the convolutional kernels.\n",
    "        'layers': [(500, 300), (400, 200)],  # Size of dense layers.\n",
    "        #'reduction': [16, 32],  # Divides the number of dense neurons in the first layer of the attention block.\n",
    "        'n_epochs': [2000, 1500, 1000],  # Number of epochs to train the model.\n",
    "        'batch_size': [16, 32],  # Number of samples per update.\n",
    "        'dropout': [0.5, 0.3, 0.2],  # Dropout rate, in the range [0, 1).\n",
    "        'dilation': [1, 2],  # Dilation value.\n",
    "        'activation': ['sigmoid', 'relu'],  # Activation function for the last output layer.\n",
    "        'loss': ['binary_crossentropy', 'categorical_crossentropy'],  # Loss function for the classifier.\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)],  # Gradient updating function for the classifier.\n",
    "        'use_bias': [True, False],  # Whether to use bias in the output dense layer.\n",
    "        'use_rp': [True, False],  # Whether to use random projections.\n",
    "        'use_att': [True, False],  # Whether to use self-attention.\n",
    "        'use_lstm': [True, False],  # Whether to use an LSTM layer.\n",
    "        'use_cnn': [True, False],  # Whether to use a CNN layer.\n",
    "        'verbose': [ True],  # Whether to output extra information.\n",
    "        'random_state': [random_state_val],  # Seed for random.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.optimizers.Adadelta(), keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3160e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.optimizers.Adam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d51a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CanonicalIntervalForest_class():\n",
    "    algorithm_implementation = CanonicalIntervalForest\n",
    "\n",
    "    method_name = 'CanonicalIntervalForest'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_estimators': [100, 200, 300],              # Number of estimators to build for the ensemble\n",
    "        'n_intervals': [10, 20, 30],                  # Number of intervals to extract per tree\n",
    "        'att_subsample_size': [4, 8, 12],             # Number of catch22 or summary statistic attributes to subsample per tree\n",
    "        'min_interval': [2, 3, 4],                    # Minimum length of an interval\n",
    "        'max_interval': [None, 10, 20],               # Maximum length of an interval\n",
    "        'base_estimator': [CanonicalIntervalForest()],# Base estimator for the ensemble\n",
    "        'n_jobs': [-1],                           # The number of jobs to run in parallel for both `fit` and `predict`\n",
    "        'random_state': [random_state_val],                   # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe17476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionTimeClassifier_class():\n",
    "    algorithm_implementation = InceptionTimeClassifier\n",
    "\n",
    "    method_name = 'InceptionTimeClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_classifiers': [3, 5, 7],               # Number of Inception models used for the Ensemble\n",
    "        'depth': [4, 6, 8],                       # Number of inception modules used\n",
    "        'nb_filters': [32, 64, 128],              # Number of filters used in one inception module\n",
    "        'nb_conv_per_layer': [3, 4],              # Number of convolution layers in each inception module\n",
    "        'kernel_size': [30, 40, 50],              # Head kernel size used for each inception module\n",
    "#         'use_max_pooling': [True, False],         # Whether to use max pooling layer in inception modules #will throw error\n",
    "        'max_pool_size': [2, 3],                  # Size of the max pooling layer\n",
    "        'strides': [1, 2],                        # Strides of kernels in convolution layers for each inception module\n",
    "        'dilation_rate': [1, 2],                  # Dilation rate of convolutions in each inception module\n",
    "        'padding': ['same', 'valid'],             # Type of padding used for convolution for each inception module\n",
    "        'activation': ['relu', 'tanh'],           # Activation function used in each inception module\n",
    "        'use_bias': [True],                # Whether to use bias values in each inception module\n",
    "        'use_residual': [True, False],            # Whether to use residual connections all over Inception\n",
    "        'use_bottleneck': [True, False],          # Whether to use bottlenecks all over Inception\n",
    "        'bottleneck_size': [16, 32],              # Bottleneck size in case use_bottleneck = True\n",
    "        'use_custom_filters': [True, False],      # Whether to use custom filters in the first inception module\n",
    "        'batch_size': [32, 64],                   # Number of samples per gradient update\n",
    "        'use_mini_batch_size': [True, False],     # Whether to use the mini batch size formula Wang et al.\n",
    "        'n_epochs': [1000, 1500, 2000],           # Number of epochs to train the model\n",
    "        'callbacks': [None],                      # List of tf.keras.callbacks.Callback objects\n",
    "        #'file_path': ['./'],                      # File path when saving model_Checkpoint callback\n",
    "        #'save_best_model': [True, False],         # Whether to save the best model\n",
    "        #'save_last_model': [True, False],         # Whether to save the last model, last epoch trained\n",
    "        #'best_file_name': ['best_model'],         # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model'],         # Name of the file of the last model\n",
    "        'random_state': [random_state_val],                  # Seed for random actions\n",
    "        'verbose': [ True],                 # Whether to output extra information\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)],                    # Keras optimizer\n",
    "        'loss': ['categorical_crossentropy'],     # Keras loss\n",
    "        'metrics': ['accuracy'],            # Keras metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7844ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUSE_class():\n",
    "    algorithm_implementation = MUSE\n",
    "\n",
    "    method_name = 'MUSE'\n",
    "\n",
    "    parameter_space = {\n",
    "        'anova': [True],                         # If True, Fourier coefficient selection is done via a one-way ANOVA test #set variance or anova true\n",
    "        #'variance': [True, False],                      # If True, Fourier coefficient selection is done via the largest variance\n",
    "        'bigrams': [True, False],                       # Whether to create bigrams of SFA words\n",
    "        'window_inc': [2, 4],                           # Increment used to determine the next window size for BoP model\n",
    "        'alphabet_size': [4, 6, 8],                     # Number of possible letters (values) for each word\n",
    "        'use_first_order_differences': [True, False],   # If True, adds the first order differences of each dimension to the data\n",
    "        'feature_selection': ['chi2', 'none', 'random'], # Sets the feature selection strategy to be used\n",
    "        'p_threshold': [0.01, 0.05, 0.1],               # P-value threshold for chi-squared test on bag-of-words\n",
    "        'support_probabilities': [True, False],         # If True, trains a LogisticRegression to support predict_proba()\n",
    "        'n_jobs': [-1],                             # Number of jobs to run in parallel for fit and predict\n",
    "        'random_state': [random_state_val],                       # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d46948",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndividualInceptionClassifier_class():\n",
    "    algorithm_implementation = IndividualInceptionClassifier\n",
    "\n",
    "    method_name = 'IndividualInceptionClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'depth': [6, 8, 10],\n",
    "        'nb_filters': [32, 64, 128],\n",
    "        'nb_conv_per_layer': [3, 4, 5],\n",
    "        'kernel_size': [30, 40, 50],\n",
    "        #'use_max_pooling': [True, False],\n",
    "        #'max_pool_size': [2, 3, 4],\n",
    "        'strides': [1, 2],\n",
    "        'dilation_rate': [1, 2],\n",
    "        'padding': ['same', 'valid'],\n",
    "        'activation': ['relu', 'elu'],\n",
    "        'use_bias': [True, False],\n",
    "        'use_residual': [True, False],\n",
    "        'use_bottleneck': [True, False],\n",
    "        'bottleneck_size': [16, 32, 64],\n",
    "        'use_custom_filters': [True, False],\n",
    "        'batch_size': [32, 64, 128],\n",
    "        'use_mini_batch_size': [True, False],\n",
    "        'n_epochs': [1000, 1500, 2000],\n",
    "        #'callbacks': [None, [ReduceOnPlateau(), ModelCheckpoint()]],\n",
    "        #'file_path': ['./', '/path/to/save'],\n",
    "        #'save_best_model': [True, False],\n",
    "        #'save_last_model': [True, False],\n",
    "        #'best_file_name': ['best_model', 'model_best'],\n",
    "        #'last_file_name': ['last_model', 'model_last'],\n",
    "        'random_state': [0],\n",
    "        'verbose': [True],\n",
    "        #'optimizer': [Adam(), RMSprop(), SGD()],\n",
    "        'loss': ['categorical_crossentropy', 'binary_crossentropy'],\n",
    "        'metrics': [None, 'accuracy'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d717974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#non mutli\n",
    "class ShapeDTW_class():\n",
    "    \n",
    "    algorithm_implementation = ShapeDTW\n",
    "    \n",
    "    method_name = 'ShapeDTW'\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        'n_neighbors': [1],\n",
    "        'subsequence_length': [f'sqrt(n_timepoints)'], #is dynamic?\n",
    "        'shape_descriptor_function': ['raw'],\n",
    "        #'params': [None],\n",
    "        'shape_descriptor_functions': [['raw', 'derivative']],\n",
    "        'metric_params': [None]\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier_class():\n",
    "    algorithm_implementation = CNNClassifier\n",
    "    \n",
    "    method_name = 'CNNClassifier'\n",
    "    \n",
    "    #nb revisit params dependent on width of the dataset\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        #'n_layers': [2, 3, 4],\n",
    "        #'kernel_size': [3, 5, 7],\n",
    "        #'n_filters': [[6, 12], [8, 16], [10, 20]],\n",
    "        #'avg_pool_size': [2, 3, 4],\n",
    "        'activation': ['sigmoid', 'relu'],\n",
    "        'padding': ['valid'],\n",
    "        #'strides': [1, 2],\n",
    "        'dilation_rate': [1, 2],\n",
    "        'use_bias': [True],\n",
    "        'random_state': [random_state_val],\n",
    "        'n_epochs': [1000, 2000, 3000],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'verbose': [ True],\n",
    "        'loss': ['binary_crossentropy'],\n",
    "        'metrics': ['accuracy'],\n",
    "        #'save_best_model': [True, False],\n",
    "        #'save_last_model': [True, False],\n",
    "        #'best_file_name': ['best_model', 'top_model'],\n",
    "        #'last_file_name': ['last_model', 'final_model'], \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNClassifier_class():\n",
    "    algorithm_implementation = FCNClassifier\n",
    "    \n",
    "    method_name = 'FCNClassifier'\n",
    "    \n",
    "    parameter_space = {\n",
    "        'n_layers': [3],\n",
    "        'n_filters': [128, 256, 128],\n",
    "        'kernel_size': [8, 5, 3],\n",
    "        'dilation_rate': [1],\n",
    "        'strides': [1],\n",
    "        'padding': ['same'],\n",
    "        'activation': ['relu'],\n",
    "        'use_bias': [True],\n",
    "        'n_epochs': [2000, 1000],\n",
    "        'batch_size': [16],\n",
    "        'use_mini_batch_size': [True],\n",
    "        'random_state': [random_state_val],\n",
    "        'verbose': [1],\n",
    "        'loss': ['categorical_crossentropy'],\n",
    "        'metrics': [None],\n",
    "        'optimizer': [keras.optimizers.Adam(0.01), keras.optimizers.SGD(0.01)], \n",
    "        #'file_path': ['./'],\n",
    "        #'save_best_model': [False],\n",
    "        #'save_last_model': [False],\n",
    "        #'best_file_name': ['best_model'],\n",
    "        #'last_file_name': ['last_model'],\n",
    "        #'callbacks': [None]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af458c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIVECOTEV2_class():\n",
    "    algorithm_implementation = HIVECOTEV2\n",
    "\n",
    "    method_name = 'HIVECOTEV2'\n",
    "    \n",
    "    #NB should add multi sets of params somehow to ensemble components!\n",
    "\n",
    "    parameter_space = {\n",
    "        'stc_params': [None, ],  # Parameters for the ShapeletTransformClassifier module. If None, uses the default parameters with a 2-hour transform contract.\n",
    "        'drcif_params': [None, ],  # Parameters for the DrCIF module. If None, uses the default parameters with n_estimators set to 500.\n",
    "        'arsenal_params': [None, ],  # Parameters for the Arsenal module. If None, uses the default parameters.\n",
    "        'tde_params': [None, ],  # Parameters for the TemporalDictionaryEnsemble module. If None, uses the default parameters.\n",
    "        'time_limit_in_minutes': time_limit_param,  # Time contract to limit build time in minutes, overriding n_estimators/n_parameter_samples for each component. Default of 0 means n_estimators/n_parameter_samples for each component is used.\n",
    "        'save_component_probas': [False],  # When predict/predict_proba is called, save each HIVE-COTEV2 component probability predictions in component_probas.\n",
    "        'verbose': [verbose_param],  # Level of output printed to the console (for information only).\n",
    "        'random_state': [random_state_val],  # If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "        'n_jobs': [-1],  # The number of jobs to run in parallel for both fit and predict. -1 means using all processors.\n",
    "        #'parallel_backend': [ 'multiprocessing'], #None, 'loky',  , 'threading'# Specify the parallelization backend implementation in joblib for Catch22, if None a ‘prefer’ value of “threads” is used by default. Valid options are “loky”, “multiprocessing”, “threading” or a custom backend. See the joblib Parallel documentation for more details.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1f5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.feature_based import Catch22Classifier\n",
    "\n",
    "\n",
    "class Catch22Classifier_class():\n",
    "    algorithm_implementation = Catch22Classifier\n",
    "\n",
    "    method_name = 'Catch22Classifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        #'features': [\"all\", [\"DN_HistogramMode_5\", \"DN_HistogramMode_10\"], ...],  # List of catch22 features to extract\n",
    "        #'features':['all'],\n",
    "        #'catch24': [True, False],                                                  # Extract mean, std, and 22 Catch22 features\n",
    "        'outlier_norm': [True, False],                                             # Normalize during outlier Catch22 features\n",
    "        'replace_nans': [True, False],                                             # Replace NaN/inf values from the transform\n",
    "        #'use_pycatch22': [True, False],                                            # Use C-based pycatch22 implementation\n",
    "        'estimator': [RandomForestClassifier(n_estimators=200),                  # Sklearn estimator for building the model\n",
    "                      DecisionTreeClassifier()],                              # Add more estimators if desired\n",
    "        'random_state': [random_state_val],                                                # Random seed for random number generator\n",
    "        'n_jobs': [-1],                                                         # Number of jobs for parallel processing\n",
    "       # 'parallel_backend': [None, \"loky\", \"multiprocessing\", \"threading\"],        # Parallelization backend options\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_function_arguments(func):\n",
    "    \"\"\"\n",
    "    Get a list of arguments that a function accepts.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to inspect.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of argument names.\n",
    "    \"\"\"\n",
    "    signature = inspect.signature(func)\n",
    "    return [param.name for param in signature.parameters.values()]\n",
    "\n",
    "arguments_list = get_function_arguments(aeon.classification.dictionary_based._tde.TemporalDictionaryEnsemble)\n",
    "print(arguments_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDictionaryEnsemble_class():\n",
    "    algorithm_implementation = aeon.classification.dictionary_based._tde.TemporalDictionaryEnsemble\n",
    "    \n",
    "\n",
    "    method_name = 'TemporalDictionaryEnsemble'\n",
    "\n",
    "    parameter_space = {\n",
    "#         'n_parameter_samples': [100, 250, 500],          # Number of parameter combinations to consider\n",
    "#         'max_ensemble_size': [25, 50, 100],              # Maximum number of estimators in the ensemble\n",
    "#         'max_win_len_prop': [0.5, 1.0],                 # Maximum window length as a proportion of series length\n",
    "#         'min_window': [5, 10, 15],                      # Minimum window length\n",
    "#         'randomly_selected_params': [25, 50, 75],       # Number of randomly selected parameters before GP parameter selection\n",
    "#         'bigrams': [True, False, None],                 # Whether to use bigrams\n",
    "#         'dim_threshold': [0.7, 0.85, 0.95],             # Dimension accuracy threshold for multivariate data\n",
    "#         'max_dims': [10, 20, 30],                       # Max number of dimensions per classifier for multivariate data\n",
    "#         'time_limit_in_minutes': time_limit_param,           # Time contract to limit build time in minutes\n",
    "#         'contract_max_n_parameter_samples': [100, 250, 500],  # Max number of parameter combinations to consider with time limit\n",
    "#         'typed_dict': [True, False],                    # Use a numba typed Dict to store word counts\n",
    "        #'save_train_predictions': [True, False],        # Save the ensemble member train predictions in fit\n",
    "        'n_jobs': [-1],                             # Number of jobs for parallel processing\n",
    "        'random_state': [random_state_val],                     # Random seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arsenal_class():\n",
    "    algorithm_implementation = Arsenal\n",
    "\n",
    "    method_name = 'Arsenal'\n",
    "\n",
    "    parameter_space = {\n",
    "         'num_kernels': [1000, 2000, 3000],  # Number of kernels for each ROCKET transform.\n",
    "        'n_estimators': [3, 5, 6],  # Number of estimators to build for the ensemble.\n",
    "        'rocket_transform': [\"rocket\", \"minirocket\"],  # The type of Rocket transformer to use. #, \"multirocket\" # broken\n",
    "        # Valid inputs = [\"rocket\", \"minirocket\", \"multirocket\"].\n",
    "        'max_dilations_per_kernel': [16, 32, 64],  # MiniRocket and MultiRocket only. The maximum number of dilations per kernel.\n",
    "        'n_features_per_kernel': [3, 4, 5],  # MultiRocket only. The number of features per kernel.\n",
    "        'time_limit_in_minutes': time_limit_param,  # Time contract to limit build time in minutes, overriding n_estimators. Default of 0 means n_estimators is used.\n",
    "        'contract_max_n_estimators': [50, 100, 150],  # Max number of estimators when time_limit_in_minutes is set.\n",
    "        #'save_transformed_data': [True, False],  # Save the data transformed in fit for use in _get_train_probs.\n",
    "        'n_jobs': [-1],  # The number of jobs to run in parallel for both fit and predict. -1 means using all processors.\n",
    "        'random_state': [random_state_val],  # Seed for random number generation.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e35be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning complex method\n",
    "class DrCIF_class():\n",
    "    algorithm_implementation = DrCIF\n",
    "\n",
    "    method_name = 'DrCIF'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_estimators': [100, 200, 300],              # Number of estimators to build for the ensemble\n",
    "        'n_intervals': [10, 20, 30],                  # Number of intervals to extract per representation per tree\n",
    "        'att_subsample_size': [4, 8, 12],             # Number of catch22 or summary statistic attributes to subsample per tree\n",
    "        'min_interval': [2, 3, 4],                    # Minimum length of an interval per representation\n",
    "        'max_interval': [None, 10, 20],               # Maximum length of an interval per representation\n",
    "        'base_estimator': [  # Base estimator for the ensemble\n",
    "                           \"DTC\", \"CIT\"],             # Add more base estimator options if desired\n",
    "        'time_limit_in_minutes': [2],#time_limit_param,         # Time contract to limit build time in minutes\n",
    "        'contract_max_n_estimators': [100, 500, 1000],# Max number of estimators when time_limit_in_minutes is set\n",
    "        #'save_transformed_data': [True, False],       # Save the transformed dataset for use in _get_train_probs\n",
    "        'n_jobs': [-1],                           # Number of jobs for parallel processing\n",
    "        'random_state': [random_state_val],                   # Random seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderClassifier_class():\n",
    "    algorithm_implementation = EncoderClassifier\n",
    "\n",
    "    method_name = 'EncoderClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'kernel_size': [[5], [11], [21], [5, 11], [5, 21], [11, 21], [5, 11, 21]],  # Specifying the length of the 1D convolution windows\n",
    "        'n_filters': [[128], [256], [512], [128, 256], [128, 512], [256, 512], [128, 256, 512]],  # Specifying the number of 1D convolution filters used for each layer\n",
    "        'max_pool_size': [2, 3],                          # Size of the max pooling windows\n",
    "        'activation': ['sigmoid', 'relu', 'tanh'],        # Keras activation function\n",
    "        'dropout_proba': [0.0, 0.2, 0.5],                 # Dropout layer probability\n",
    "        'padding': ['same', 'valid'],                     # Type of padding used for 1D convolution\n",
    "        'strides': [1, 2],                               # Sliding rate of the 1D convolution filter\n",
    "        'fc_units': [128, 256, 512],                      # Number of units in the hidden fully connected layer\n",
    "        #'file_path': ['./', './models/'],                # File path when saving the model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'best_classifier'],  # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'last_classifier'],  # Name of the file of the last model\n",
    "        'random_state': [random_state_val],                         # Seed for any needed random actions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderClassifier_class():\n",
    "    algorithm_implementation = EncoderClassifier\n",
    "\n",
    "    method_name = 'EncoderClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'kernel_size': [[5], ],  # Specifying the length of the 1D convolution windows\n",
    "        'n_filters': [[128], ],  # Specifying the number of 1D convolution filters used for each layer\n",
    "        'max_pool_size': [2, 3],                          # Size of the max pooling windows\n",
    "        'activation': ['sigmoid', 'relu', 'tanh'],        # Keras activation function\n",
    "        'dropout_proba': [0.0, 0.2, 0.5],                 # Dropout layer probability\n",
    "        'padding': ['same', 'valid'],                     # Type of padding used for 1D convolution\n",
    "        'strides': [1, 2],                               # Sliding rate of the 1D convolution filter\n",
    "        'fc_units': [128, 256, 512],                      # Number of units in the hidden fully connected layer\n",
    "        #'file_path': ['./', './models/'],                # File path when saving the model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'best_classifier'],  # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'last_classifier'],  # Name of the file of the last model\n",
    "        'random_state': [random_state_val],                         # Seed for any needed random actions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45cae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureClassifier_class():\n",
    "    algorithm_implementation = SignatureClassifier\n",
    "\n",
    "    method_name = 'SignatureClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        #'estimator': [RandomForestClassifier(n_estimators=100), DecisionTreeClassifier()],\n",
    "       # 'augmentation_list': [('basepoint', 'addtime'), ('addtime',)],\n",
    "        #'window_name': ['dyadic', 'sliding', 'expanding'],\n",
    "        #'window_depth': [2, 3, 4],\n",
    "#        'window_length': [None],\n",
    "#         'window_step': [ 1, 5],\n",
    "#         'rescaling': [None, 'standard', 'min-max'],\n",
    "#         'sig_tfm': ['signature', 'logsignature'],\n",
    "#        'depth': [52],\n",
    "        'random_state': [random_state_val],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.interval_based import DrCIFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043705c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrCIFClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrCIFClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f661e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(DrCIFClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warning complex method\n",
    "class DrCIF_class():\n",
    "    algorithm_implementation = DrCIFClassifier\n",
    "\n",
    "    method_name = 'DrCIF'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_estimators': [100, 200, 300],              # Number of estimators to build for the ensemble\n",
    "        'n_intervals': [10, 20, 30],                  # Number of intervals to extract per representation per tree\n",
    "        'att_subsample_size': [4, 8, 12],             # Number of catch22 or summary statistic attributes to subsample per tree\n",
    "        'min_interval_length': [2, 3, 4],                    # Minimum length of an interval per representation\n",
    "        'max_interval_length': [10, 20],               # Maximum length of an interval per representation\n",
    "        'base_estimator': [  # Base estimator for the ensemble\n",
    "                           None],             # Add more base estimator options if desired #defaults to tree \n",
    "        'time_limit_in_minutes': time_limit_param,         # Time contract to limit build time in minutes\n",
    "        'contract_max_n_estimators': [100, 500, 1000],# Max number of estimators when time_limit_in_minutes is set\n",
    "        #'save_transformed_data': [True, False],       # Save the transformed dataset for use in _get_train_probs\n",
    "        'n_jobs': [n_jobs_model_val],                           # Number of jobs for parallel processing\n",
    "        'random_state': [random_state_val],                   # Random seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bd1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs_model_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSFreshClassifier_class():\n",
    "    algorithm_implementation = TSFreshClassifier\n",
    "\n",
    "    method_name = 'TSFreshClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'default_fc_parameters': ['minimal', 'efficient', 'comprehensive'],   # Set of TSFresh features to be extracted\n",
    "        'relevant_feature_extractor': [True, False],                          # Whether to remove irrelevant features using the FRESH algorithm\n",
    "        'estimator': [None, RandomForestClassifier(n_estimators=200)],       # An sklearn estimator to be built using the transformed data\n",
    "        'verbose': [verbose_param],                                                 # Level of output printed to the console\n",
    "        'n_jobs': [n_jobs_model_val],                                                    # Number of jobs to run in parallel for fit and predict\n",
    "        'chunksize': [None, 10, 100],                                         # Number of series processed in each parallel TSFresh job\n",
    "        'random_state': [random_state_val],                                              # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c70953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unknown if ts class model\n",
    "class OrdinalTDE_class():\n",
    "    algorithm_implementation = OrdinalTDE\n",
    "\n",
    "    method_name = 'OrdinalTDE'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_parameter_samples': [100, 250, 500],                # Number of parameter combinations to consider for the final ensemble\n",
    "        'max_ensemble_size': [30, 50, 100],                     # Maximum number of estimators in the ensemble\n",
    "        'max_win_len_prop': [0.8, 1.0],                        # Maximum window length as a proportion of series length\n",
    "        'min_window': [5, 10, 15],                             # Minimum window length\n",
    "        'randomly_selected_params': [30, 50, 70],              # Number of parameters randomly selected before Gaussian process parameter selection\n",
    "        'bigrams': [True, False],                        # Whether to use bigrams\n",
    "        'dim_threshold': [0.75, 0.85, 0.95],                   # Dimension accuracy threshold for multivariate data\n",
    "        'max_dims': [10, 20, 30],                              # Max number of dimensions per classifier for multivariate data\n",
    "        'time_limit_in_minutes': time_limit_param,                  # Time contract to limit build time in minutes\n",
    "        'contract_max_n_parameter_samples': [1000, 2000],      # Max number of parameter combinations when time_limit_in_minutes is set\n",
    "        'typed_dict': [True, False],                           # Whether to use numba typed Dict to store word counts\n",
    "        #'save_train_predictions': [True, False],               # Save ensemble member train predictions in fit for LOOCV\n",
    "        'n_jobs': [n_jobs_model_val],                                     # Number of jobs to run in parallel for fit and predict\n",
    "        'random_state': [random_state_val],                              # Seed for random number generation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8c07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.distance_based import ElasticEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5241750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74858a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_combinations(items_list):\n",
    "    result = []\n",
    "    for i in range(1, len(items_list) + 1):\n",
    "        for combo in combinations(items_list, i):\n",
    "            result.append(list(combo))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3642c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ['euclidean', 'dtw', 'wdtw', 'ddtw', 'dwdtw', 'lcss', 'erp', 'msm', 'twe']\n",
    "\n",
    "measures\n",
    "len(all_combinations(measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = all_combinations(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f72083",
   "metadata": {},
   "outputs": [],
   "source": [
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_combinations(items_list):\n",
    "    result = []\n",
    "    for i in range(1, len(items_list) + 1):\n",
    "        for combo in combinations(items_list, i):\n",
    "            result.append(list(combo))\n",
    "    return result\n",
    "\n",
    "\n",
    "class ElasticEnsemble_class:\n",
    "\n",
    "    # measures = ['euclidean', 'dtw', 'wdtw', 'ddtw', 'dwdtw', 'lcss', 'erp', 'msm', 'twe'] #none supported\n",
    "\n",
    "    # measures = [ ]\n",
    "\n",
    "    # res = all_combinations(measures)\n",
    "\n",
    "    # res.extend(['all'])\n",
    "\n",
    "    algorithm_implementation = ElasticEnsemble\n",
    "\n",
    "    method_name = \"ElasticEnsemble\"\n",
    "\n",
    "    # ['euclidean', 'dtw', 'wdtw', 'ddtw', 'dwdtw', 'lcss', 'erp', 'msm', 'twe']\n",
    "\n",
    "    parameter_space = {\n",
    "        #        'distance_measures': [ 'all'],\n",
    "        #'distance_measures': [],\n",
    "        \"proportion_of_param_options\": [1.0, 0.8, 0.6],\n",
    "        \"proportion_train_in_param_finding\": [1.0, 0.8, 0.6],\n",
    "        \"proportion_train_for_test\": [1.0, 0.8, 0.6],\n",
    "        \"n_jobs\": [n_jobs_model_val],  # -1 means using all processors.\n",
    "        #'random_state': [0],  # The random seed.\n",
    "        #        'verbose': [1],  # If >0, then prints out debug information. #this is throwing an error\n",
    "        \"majority_vote\": [\n",
    "            False,\n",
    "            True,\n",
    "        ],  # Whether to use majority vote or weighted vote.\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aed5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier_class():\n",
    "    algorithm_implementation = ResNetClassifier\n",
    "\n",
    "    method_name = 'ResNetClassifier'\n",
    "\n",
    "    parameter_space = {\n",
    "        'n_residual_blocks': [0, 1, 2, 3, 4],            # The number of residual blocks of ResNet's model\n",
    "        'n_conv_per_residual_block': [ 1, 2, 3, 4],     # The number of convolution blocks in each residual block\n",
    "        'n_filters': [None, 64, 128, 256],               # The number of convolution filters for all the convolution layers in the same residual block\n",
    "        #'kernel_sizes': [3, 5, 7],                 # The kernel size of all the convolution layers in one residual block\n",
    "        'strides': [1],                         # The strides of convolution kernels in each of the convolution layers in one residual block\n",
    "        'dilation_rate': [ 1, 2],                   # The dilation rate of the convolution layers in one residual block\n",
    "        'padding': ['same', 'valid'],              # The type of padding used in the convolution layers in one residual block\n",
    "        'activation': ['relu', 'tanh'],            # Keras activation used in the convolution layers in one residual block\n",
    "        'use_bias': [True, False],                 # Condition on whether or not to use bias values in the convolution layers in one residual block\n",
    "        'n_epochs': log_epoch,            # The number of epochs to train the model\n",
    "        'batch_size': [16, 32, 64],                # The number of samples per gradient update\n",
    "        'use_mini_batch_size': [True, False],      # Condition on using the mini-batch size formula Wang et al.\n",
    "        'callbacks': [None],                       # List of tf.keras.callbacks.Callback objects\n",
    "        #'file_path': ['./', './models/'],          # File path when saving model_Checkpoint callback\n",
    "        'save_best_model': [False],                # Whether or not to save the best model\n",
    "        'save_last_model': [False],                # Whether or not to save the last model\n",
    "        #'best_file_name': ['best_model', 'best_classifier'],  # Name of the file of the best model\n",
    "        #'last_file_name': ['last_model', 'last_classifier'],  # Name of the file of the last model\n",
    "        'verbose': [verbose_param],                  # Whether to output extra information\n",
    "        'loss': [ 'categorical_crossentropy'],  # Fit parameter for the keras model # 'mean_squared_error',\n",
    "        'optimizer': [None, keras.optimizers.Adadelta(), keras.optimizers.Adam()],                  # Optimizer for the model\n",
    "        'metrics': [None, 'accuracy', 'mae'],  # List of strings for metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ace310",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class_list = [\n",
    "        FCNClassifier_class,\n",
    "        KNeighborsTimeSeriesClassifier_class,\n",
    "        ElasticEnsemble_class,\n",
    "        #ShapeDTW_class,#uni\n",
    "        #HIVECOTEV1_class, #uni\n",
    "        HIVECOTEV2_class,\n",
    "        RocketClassifier_class,\n",
    "        CNNClassifier_class,\n",
    "        \n",
    "        MLPClassifier_class,\n",
    "        TapNetClassifier_class,\n",
    "        Arsenal_class,\n",
    "        #Arsenal_class,\n",
    "        IndividualTDE_class,\n",
    "        #CanonicalIntervalForest_class,\n",
    "        Catch22Classifier_class,\n",
    "        DrCIF_class,\n",
    "        TemporalDictionaryEnsemble_class,\n",
    "        EncoderClassifier_class,\n",
    "        ResNetClassifier_class,\n",
    "        FreshPRINCEClassifier_class,\n",
    "        InceptionTimeClassifier_class,\n",
    "        OrdinalTDE_class,\n",
    "        #ContractableBOSS_class, #multi series\n",
    "        MUSE_class,\n",
    "        TSFreshClassifier_class,\n",
    "        SummaryClassifier_class,\n",
    "        SignatureClassifier_class,# param problem, solution unknown, troubleseek? maxed multivar f limit?\n",
    "        IndividualInceptionClassifier_class\n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0eeee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(arg_list)):\n",
    "    \n",
    "    print(i, arg_list[i][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e76e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_class_list)):\n",
    "    print(model_class_list[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e166c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = 14\n",
    "\n",
    "class_name = model_class_list[idx]\n",
    "\n",
    "args = (\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test, \n",
    "                class_name.algorithm_implementation,\n",
    "                class_name.parameter_space,\n",
    "                class_name.method_name,\n",
    "                X,\n",
    "                y,\n",
    "                #class_name.parameter_space\n",
    "                \n",
    "            )\n",
    "\n",
    "print(f\"Test args set {class_name.method_name}\")\n",
    "\n",
    "sub_sample_param_space_pct = 0.000001 #should force 2 max\n",
    "\n",
    "parameter_space = class_name.parameter_space\n",
    "\n",
    "n_iter_v = int(sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2\n",
    "\n",
    "n_iter_v\n",
    "\n",
    "#%pip install --upgrade tensorflow\n",
    "\n",
    "import itertools\n",
    "\n",
    "def get_single_permutation(grid):\n",
    "    # Extract the values from the dictionary grid\n",
    "    values = list(grid.values())\n",
    "    \n",
    "    # Get the permutation iterator\n",
    "    permutation_iterator = itertools.permutations(values)\n",
    "    \n",
    "    # Get the first permutation from the iterator\n",
    "    single_permutation = next(permutation_iterator)\n",
    "    \n",
    "    # Convert the permutation tuple back to a dictionary\n",
    "    result = {key: value for key, value in zip(grid.keys(), single_permutation)}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "grid = class_name.parameter_space\n",
    "\n",
    "single_permutation = get_single_permutation(grid)\n",
    "print(single_permutation)\n",
    "\n",
    "\n",
    "arbitrary_permutation = {}\n",
    "for key, values in parameter_space.items():\n",
    "    arbitrary_permutation[key] = random.choice(values)\n",
    "\n",
    "print(arbitrary_permutation)\n",
    "\n",
    "class_name.algorithm_implementation\n",
    "\n",
    "model = class_name.algorithm_implementation(**arbitrary_permutation)\n",
    "\n",
    "arbitrary_permutation\n",
    "\n",
    "model\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 3: Compute the Area Under the ROC Curve (AUC) using the predicted labels and true labels\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1480ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_class_list)):\n",
    "    print(model_class_list[i], i)\n",
    "    print(model_class_list[i].algorithm_implementation().get_tags())\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b62f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f923631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26d0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample_param_space_pct = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353be26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = class_name.parameter_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65690696",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_v = int(sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c6ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcd6b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c52713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def get_single_permutation(grid):\n",
    "    # Extract the values from the dictionary grid\n",
    "    values = list(grid.values())\n",
    "    \n",
    "    # Get the permutation iterator\n",
    "    permutation_iterator = itertools.permutations(values)\n",
    "    \n",
    "    # Get the first permutation from the iterator\n",
    "    single_permutation = next(permutation_iterator)\n",
    "    \n",
    "    # Convert the permutation tuple back to a dictionary\n",
    "    result = {key: value for key, value in zip(grid.keys(), single_permutation)}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "grid = class_name.parameter_space\n",
    "\n",
    "single_permutation = get_single_permutation(grid)\n",
    "print(single_permutation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_permutation = {}\n",
    "for key, values in parameter_space.items():\n",
    "    arbitrary_permutation[key] = random.choice(values)\n",
    "\n",
    "print(arbitrary_permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name.algorithm_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172045b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = class_name.algorithm_implementation(**arbitrary_permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0757754",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc392289",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee3697",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc4375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step 3: Compute the Area Under the ROC Curve (AUC) using the predicted labels and true labels\n",
    "auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd810b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_n_jobs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f196494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cv(X_train, Y_train, x_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict):\n",
    "    \n",
    "    print(f\"grid searching {algorithm_implementation}\")\n",
    "    \n",
    "    gpu_model_list = ['FCNClassifier']\n",
    "    \n",
    "    ## grid n jobs overloading gpu?\n",
    "    if(algorithm_implementation in gpu_model_list ):\n",
    "        grid_n_jobs = 1\n",
    "        \n",
    "    grid_n_jobs = -1\n",
    "    \n",
    "    current_algorithm = algorithm_implementation()\n",
    "    \n",
    "    print(current_algorithm)\n",
    "    \n",
    "    parameters = parameter_space\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    #Grid search over hyperparameter space, randomised. \n",
    "    if(random_grid_search):\n",
    "        n_iter_v = int(sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2\n",
    "        \n",
    "        grid= RandomizedSearchCV(current_algorithm, parameters,\n",
    "                                 verbose=1, cv=[(slice(None), slice(None))],\n",
    "                                 n_jobs =grid_n_jobs, n_iter = n_iter_v, error_score=np.nan)\n",
    "        \n",
    "        print(f\"grid searching  hyperparameter space, randomised grid {grid}\")\n",
    "        \n",
    "    else:   \n",
    "        grid = GridSearchCV(\n",
    "            current_algorithm, parameters, verbose=1, cv=[(slice(None), slice(None))], n_jobs =grid_n_jobs,\n",
    "            error_score=np.nan\n",
    "        )  # Negate CV in param search for speed\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    pg = ParameterGrid(parameter_space)\n",
    "    pg = len(pg)\n",
    "    print(pg)\n",
    "    if pg > 100000:\n",
    "        print(\"grid too large\", str(pg))\n",
    "        raise Exception(\"grid too large\", str(pg))\n",
    "    # print(grid)\n",
    "    print(\"Full pg\", pg)\n",
    "    \n",
    "    #could sub sample X/y here in hyperparameter search. Smaller dataset == faster param search\n",
    "    if(sub_sample_hyperparameter_search):\n",
    "        X_train_sample = X_train.sample(sub_sample_hyperparameter_search_val).copy()\n",
    "        y_train_sample = y_train[X_train_sample.index].copy()\n",
    "        hp_grid_samp_len = len(X_train_sample)\n",
    "    \n",
    "        grid.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "    else:\n",
    "        hp_grid_samp_len = len(X_train)\n",
    "        #grid.fit(X, y)  \n",
    "        grid.fit(X_train, y_train) \n",
    "        \n",
    "    print(f\"Fitted {grid.best_estimator_} done\")\n",
    "        \n",
    "    current_algorithm = grid.best_estimator_\n",
    "    \n",
    "    current_algorithm.fit(X_train, y_train)    \n",
    "        \n",
    "    scores = cross_validate(\n",
    "        current_algorithm,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=metric_list,\n",
    "        cv=cv,\n",
    "        n_jobs=grid_n_jobs,  # Full CV on final best model #exp -1 was 1\n",
    "        pre_dispatch = 80, #exp,\n",
    "        error_score=np.nan\n",
    "    )\n",
    "    current_algorithm_scores = scores\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        print(\"Writing grid permutation to log\")\n",
    "        #write line to best grid scores---------------------\n",
    "        column_list = ['algorithm_implementation', 'parameter_sample','method_name', 'nb_size', 'f_list', 'auc','mcc','f1','precision','recall','accuracy',\n",
    "                       \n",
    "               'resample', 'scale', 'n_features', 'param_space_size', 'n_unique_out',\n",
    "               'outcome_var_n', 'percent_missing', 'corr', \n",
    "                       'age', 'sex', 'bmi','ethnicity', 'bloods', 'diagnostic_order',\n",
    "                      'drug_order', 'annotation_n', 'meta_sp_annotation_n',\n",
    "               'meta_sp_annotation_mrc_n','annotation_mrc_n', 'date_time_stamp',\n",
    "               'core_02','bed','vte_status','hosp_site','core_resus','news', 'hp_grid_samp_len',\n",
    "                'X_train_size', 'X_test_orig_size', 'X_test_size', 'run_time', 'n_fits', 't_fits', 'i',\n",
    "                \n",
    "              ]\n",
    "        \n",
    "        metric_names = []\n",
    "        for metric in metric_list:\n",
    "            metric_names.append(f'{metric}_m')\n",
    "            metric_names.append(f'{metric}_std')\n",
    "            \n",
    "        column_list.extend(metric_names)\n",
    "        \n",
    "\n",
    "        #column_list = column_list +['BL_' + str(x) for x in range(0, 64)]\n",
    "\n",
    "        line = pd.DataFrame( data = None, columns = column_list)\n",
    "        \n",
    "        score_successful = False\n",
    "        \n",
    "        try:\n",
    "            best_pred_orig = current_algorithm.predict(X_test_orig) #exp\n",
    "            score_successful = True\n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            print(\"failed to get best_pred_orig returning nan vector\")\n",
    "            \n",
    "            empty_vector_len = X_test_orig.shape[0]\n",
    "            \n",
    "            empty_vector = np.empty(empty_vector_len)\n",
    "\n",
    "            empty_vector[:] = np.nan\n",
    "\n",
    "            best_pred_orig = empty_vector\n",
    "            \n",
    "            score_successful = False\n",
    "        \n",
    "        \n",
    "        #best_pred_orig = grid.best_estimator_.predict(X_test_orig)\n",
    "        if(score_successful):\n",
    "            auc = metrics.roc_auc_score(y_test_orig, best_pred_orig)\n",
    "            mcc = matthews_corrcoef(y_test_orig, best_pred_orig)\n",
    "            f1  = f1_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            precision = precision_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            recall = recall_score(y_test_orig, best_pred_orig, average='binary')\n",
    "            accuracy = accuracy_score(y_test_orig, best_pred_orig)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "            mcc = np.nan\n",
    "            f1  = np.nan\n",
    "            precision = np.nan\n",
    "            recall = np.nan\n",
    "            accuracy = np.nan\n",
    "\n",
    "        for key in global_param_dict:\n",
    "            #print(key)\n",
    "            if key != 'data':\n",
    "                if(key in column_list):\n",
    "                    line[key] = [global_param_dict.get(key)]\n",
    "            else:\n",
    "                for key_1 in global_param_dict.get('data'):\n",
    "                    #print(key_1)\n",
    "                    if(key_1 in column_list):\n",
    "                        line[key_1] = [global_param_dict.get('data').get(key_1)]\n",
    "\n",
    "        current_f = final_column_list\n",
    "        #current_f = list(X_test.columns)\n",
    "        \n",
    "        current_f_vector = []\n",
    "        f_list = []\n",
    "        for elem in orignal_feature_names:\n",
    "            if(elem in current_f):\n",
    "                current_f_vector.append(1)\n",
    "            else:\n",
    "                current_f_vector.append(0)\n",
    "        #f_list.append(np.array(current_f_vector))\n",
    "        f_list.append(current_f_vector)\n",
    "        \n",
    "        line['algorithm_implementation'] = [algorithm_implementation]\n",
    "        line['parameter_sample'] = [current_algorithm]\n",
    "        line['method_name'] = [method_name]\n",
    "        line['nb_size'] = [sum(np.array(current_f_vector))]\n",
    "        line['n_features'] = [len(current_f_vector)]\n",
    "        line['f_list'] = [f_list]\n",
    "        line['hp_grid_samp_len'] = [hp_grid_samp_len]\n",
    "\n",
    "\n",
    "        line['auc'] = [auc]\n",
    "        line['mcc'] = [mcc]\n",
    "        line['f1'] = [f1]\n",
    "        line['precision'] = [precision]\n",
    "        line['recall'] = [recall]\n",
    "        line['accuracy'] = [accuracy]\n",
    "        \n",
    "        line['X_train_size'] = [len(X_train)]\n",
    "        line['X_test_orig_size'] = [len(X_test_orig)]\n",
    "        line['X_test_size'] = [len(X_test)]\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        line['run_time'] = int((end - start) / 60)\n",
    "        line['t_fits'] = pg\n",
    "        line['n_fits'] = n_iter_v\n",
    "        line['i'] = i\n",
    "        line['fit_time_m'] = scores['fit_time'].mean()\n",
    "        line['fit_time_std'] = scores['fit_time'].std()\n",
    "        \n",
    "        line['score_time_m'] = scores['score_time'].mean()\n",
    "        line['score_time_std'] = scores['score_time'].std()\n",
    "        \n",
    "        for metric in metric_list:\n",
    "            line[f'{metric}_m'] = scores[f'test_{metric}'].mean()\n",
    "            line[f'{metric}_std'] = scores[f'test_{metric}'].std()\n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "        display(line)\n",
    "        \n",
    "        #line['outcome_var'] = y_test.name\n",
    "\n",
    "        #line['nb_val'] = [nb_val]\n",
    "        #line['pop_val'] = [pop_val]\n",
    "        #line['g_val'] = [g_val]\n",
    "        #line['g'] = [g]\n",
    "\n",
    "\n",
    "        line[column_list].to_csv(base_project_dir + 'final_grid_score_log.csv' , mode='a', header=False, index=True)   \n",
    "        #---------------------------    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Failed to upgrade grid entry\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return (method_name, current_algorithm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class_list = [\n",
    "        FCNClassifier_class,\n",
    "        KNeighborsTimeSeriesClassifier_class,\n",
    "        HIVECOTEV2_class,\n",
    "        MLPClassifier_class,\n",
    "        TapNetClassifier_class,\n",
    "        Arsenal_class,\n",
    "        IndividualTDE_class,\n",
    "        Catch22Classifier_class,\n",
    "        DrCIF_class,\n",
    "        TemporalDictionaryEnsemble_class,\n",
    "        EncoderClassifier_class,\n",
    "        ResNetClassifier_class,\n",
    "        FreshPRINCEClassifier_class,\n",
    "        InceptionTimeClassifier_class,\n",
    "        OrdinalTDE_class,\n",
    "        ContractableBOSS_class,\n",
    "        MUSE_class,\n",
    "        TSFreshClassifier_class,\n",
    "        #SummaryClassifier_class,\n",
    "        #IndividualInceptionClassifier_class,\n",
    "        \n",
    "        #CNNClassifier_class, #problem!\n",
    "        \n",
    "        #SignatureClassifier_class, # problem, troubleshoot\n",
    "        #CanonicalIntervalForest_class,# Nnot found\n",
    "        #RocketClassifier_class,\n",
    "        ElasticEnsemble_class,\n",
    "        #ShapeDTW_class, #uni variate\n",
    "        #HIVECOTEV1_class, #univariate\n",
    "        \n",
    "    ]\n",
    "\n",
    "for elem in model_class_list:\n",
    "    pg = ParameterGrid(elem.parameter_space)\n",
    "    print(elem.method_name)\n",
    "    print(len(pg))\n",
    "    for param in elem.parameter_space:\n",
    "\n",
    "        if(isinstance(elem.parameter_space.get(param), list) is False and isinstance(elem.parameter_space.get(param), np.ndarray) is False):\n",
    "            print(elem.method_name, param)\n",
    "            print(type(elem.parameter_space.get(param)))\n",
    "\n",
    "scores_tuple_list = []\n",
    "model_error_list = []\n",
    "\n",
    "\n",
    "#X_train, y_train, X_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict\n",
    "\n",
    "arg_list = []\n",
    "for model_class in model_class_list:\n",
    "\n",
    "    class_name = model_class\n",
    "\n",
    "    #grid_search_cv(X_train, Y_train, x_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict):\n",
    "\n",
    "\n",
    "    arg_list.append(\n",
    "        (\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test, \n",
    "            class_name.algorithm_implementation,\n",
    "            class_name.parameter_space,\n",
    "            class_name.method_name,\n",
    "            X,\n",
    "            y,\n",
    "            #class_name.parameter_space\n",
    "\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=18\n",
    "\n",
    "print(arg_list[k][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33a3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_cv(\n",
    "    *arg_list[k]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47cee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b91f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for k in range(0, len(arg_list)):\n",
    "\n",
    "print(\"Running: \", class_name.method_name)\n",
    "grid_search_cv(\n",
    "    *arg_list[k]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    \n",
    "    \n",
    "print(f\"Main on {len(all_df_columns)-len(drop_list)} features\")\n",
    "\n",
    "if multiprocess == False:\n",
    "    for k in range(0, len(arg_list)):\n",
    "        try:\n",
    "            print(\"Running: \", class_name.method_name)\n",
    "            grid_search_cv(\n",
    "                *arg_list[k]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "            print(\"error on \", class_name.method_name)\n",
    "            model_error_list.append((arg_list[k][0], e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17768236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6da833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb240ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search_cv(\n",
    "                    *args\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7567d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b62f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_traint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c26c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83007baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from aeon.classification.deep_learning.fcn import FCNClassifier\n",
    "from aeon.datasets import load_unit_test\n",
    "X_traint, y_traint = load_unit_test(split=\"train\", return_X_y=True)\n",
    "X_testt, y_testt = load_unit_test(split=\"test\", return_X_y=True)\n",
    "fcn = FCNClassifier(n_epochs=20,batch_size=2)  \n",
    "fcn.fit(X_traint, y_traint)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8919d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install aeon[dl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ede8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install keras==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfcf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall keras -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ef59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.layers.Conv1D(\n",
    "    1,\n",
    "    2,\n",
    "    strides=1,\n",
    "    padding='valid',\n",
    "    data_format='channels_last',\n",
    "    dilation_rate=1,\n",
    "    groups=1,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    #**kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28152796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.classification.deep_learning.fcn import FCNClassifier\n",
    "from aeon.datasets import load_unit_test\n",
    "X_traint, y_traint = load_unit_test(split=\"train\", return_X_y=True)\n",
    "X_testt, y_testt = load_unit_test(split=\"test\", return_X_y=True)\n",
    "fcn = FCNClassifier(n_epochs=20,batch_size=2)  \n",
    "fcn.fit(X_traint, y_traint)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37274d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #needs to be implemented:\n",
    "\n",
    "# col_names = X.columns\n",
    "# if scale == True:\n",
    "#     scaler = ColumnTransformer(\n",
    "#         [\n",
    "#             (\n",
    "#                 \"somename\",\n",
    "#                 StandardScaler(),\n",
    "#                 list(X.columns),\n",
    "#             )\n",
    "#         ],\n",
    "#         remainder=\"passthrough\",\n",
    "#     )\n",
    "#     X = scaler.fit_transform(X)\n",
    "\n",
    "#     X = pd.DataFrame(X, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X), len(X[0]), len(X[0][0]), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7eeecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee43b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1114b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y), len(X[0]), len(X[1]), len(X[0][0]), len(X[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e67312",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "clf.fit(X_train, y_train)  # fit the classifier on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2var 6 min full row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943928e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#X_test = np.array([[2, 2, 2, 2, 2, 2, 2], [4, 4, 4, 4, 4, 4, 4]])\n",
    "y_pred = clf.predict(X_test)  # make class predictions on new data\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184a1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix dataset grid permutations \n",
    "#need to implement a meta fit method for candidate base learners \n",
    "#first check if additional aeon methods are compatible and fit on 3 dim X_train etc. \n",
    "# check grid result report\n",
    "#fix/optimise long data slice to X_train 3 dim conversion. \n",
    "#check for variable leak/ conduct screening\n",
    "#wrap each data transformation into a I/O function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "clf.fit(X_train, y_train)  # fit the classifier on train data\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c485a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c76b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e9e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider time series ensemble method built in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063ed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aeon.registry import all_estimators\n",
    "\n",
    "all_estimators(\n",
    "    filter_tags={\"capability:multivariate\": True}, estimator_types=\"classifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484f1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "[('Arsenal', aeon.classification.convolution_based._arsenal.Arsenal),\n",
    " ('CNNClassifier', aeon.classification.deep_learning.cnn.CNNClassifier),\n",
    " ('CanonicalIntervalForest',\n",
    "  aeon.classification.interval_based._cif.CanonicalIntervalForest),\n",
    " ('Catch22Classifier',\n",
    "  aeon.classification.feature_based._catch22_classifier.Catch22Classifier),\n",
    " ('ChannelEnsembleClassifier',\n",
    "  aeon.classification.compose._channel_ensemble.ChannelEnsembleClassifier),\n",
    " ('DrCIF', aeon.classification.interval_based._drcif.DrCIF),\n",
    " ('DummyClassifier', aeon.classification._dummy.DummyClassifier),\n",
    " ('EncoderClassifier',\n",
    "  aeon.classification.deep_learning.encoder.EncoderClassifier),\n",
    " ('FCNClassifier', aeon.classification.deep_learning.fcn.FCNClassifier),\n",
    " ('FreshPRINCE', aeon.classification.feature_based._fresh_prince.FreshPRINCE),\n",
    " ('HIVECOTEV2', aeon.classification.hybrid._hivecote_v2.HIVECOTEV2),\n",
    " ('InceptionTimeClassifier',\n",
    "  aeon.classification.deep_learning.inception_time.InceptionTimeClassifier),\n",
    " ('IndividualInceptionClassifier',\n",
    "  aeon.classification.deep_learning.inception_time.IndividualInceptionClassifier),\n",
    " ('IndividualTDE', aeon.classification.dictionary_based._tde.IndividualTDE),\n",
    " ('KNeighborsTimeSeriesClassifier',\n",
    "  aeon.classification.distance_based._time_series_neighbors.KNeighborsTimeSeriesClassifier),\n",
    " ('MLPClassifier', aeon.classification.deep_learning.mlp.MLPClassifier),\n",
    " ('MUSE', aeon.classification.dictionary_based._muse.MUSE),\n",
    " ('RandomIntervalClassifier',\n",
    "  aeon.classification.interval_based._random_interval_classifier.RandomIntervalClassifier),\n",
    " ('ResNetClassifier',\n",
    "  aeon.classification.deep_learning.resnet.ResNetClassifier),\n",
    " ('RocketClassifier',\n",
    "  aeon.classification.convolution_based._rocket_classifier.RocketClassifier),\n",
    " ('ShapeletTransformClassifier',\n",
    "  aeon.classification.shapelet_based._stc.ShapeletTransformClassifier),\n",
    " ('SignatureClassifier',\n",
    "  aeon.classification.feature_based._signature_classifier.SignatureClassifier),\n",
    " ('SummaryClassifier',\n",
    "  aeon.classification.feature_based._summary_classifier.SummaryClassifier),\n",
    " ('TSFreshClassifier',\n",
    "  aeon.classification.feature_based._tsfresh_classifier.TSFreshClassifier),\n",
    " ('TapNetClassifier',\n",
    "  aeon.classification.deep_learning.tapnet.TapNetClassifier),\n",
    " ('TemporalDictionaryEnsemble',\n",
    "  aeon.classification.dictionary_based._tde.TemporalDictionaryEnsemble)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d807ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a04b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocketClassifier_class():\n",
    "    \n",
    "    algorithm_implementation = RocketClassifier()\n",
    "    \n",
    "    method_name = 'RocketClassifier'\n",
    "    \n",
    "    parameter_space = {\n",
    "        \n",
    "        'num_kernels' : [10000], # , 'cityblock' 'ctw', 'sqeuclidean','sax' 'softdtw'\n",
    "        'rocket_transform' :  ['rocket', 'minirocket', 'multirocket']# [log_med_long],,\n",
    "        \n",
    "        #'max_dilations_per_kernel' : []\n",
    "        \n",
    "        \n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbeda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_implementation = KNeighborsTimeSeriesClassifier_class.algorithm_implementation\n",
    "\n",
    "parameter_space = KNeighborsTimeSeriesClassifier_class.parameter_space\n",
    "\n",
    "method_name = KNeighborsTimeSeriesClassifier_class.method_name\n",
    "\n",
    "param_dict = KNeighborsTimeSeriesClassifier_class.parameter_space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_implementation = RocketClassifier_class.algorithm_implementation\n",
    "\n",
    "parameter_space = RocketClassifier_class.parameter_space\n",
    "\n",
    "method_name = RocketClassifier_class.method_name\n",
    "\n",
    "param_dict = RocketClassifier_class.parameter_space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731f957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid_search = True\n",
    "sub_sample_param_space_pct = 0.00001\n",
    "\n",
    "grid_n_jobs = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31098738",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sample_hyperparameter_search = False\n",
    "\n",
    "sub_sample_hyperparameter_search_val = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train.shape, X_test.shape, X_test_orig.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351dc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsTimeSeriesClassifier_class.algorithm_implementation.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7b8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocketClassifier_class.algorithm_implementation.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5d6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsTimeSeriesClassifier_class.algorithm_implementation.predict(X_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed377016",
   "metadata": {},
   "outputs": [],
   "source": [
    "RocketClassifier_class.algorithm_implementation.predict(X_test_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647927d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vector = np.empty(5)\n",
    "\n",
    "empty_vector[:] = np.nan\n",
    "\n",
    "empty_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a22b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vector = np.empty(5)\n",
    "\n",
    "empty_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df716084",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vector[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a3d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26346d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218db2de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_search_cv(X_train, y_train, X_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecade0f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"running\")\n",
    "grid_search_cv(X_train, y_train, X_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv(X_train, y_train, X_test, y_test, algorithm_implementation, parameter_space, method_name, X, y, **param_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d541666",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873bf58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_algorithm = algorithm_implementation\n",
    "\n",
    "parameters = parameter_space\n",
    "\n",
    "\n",
    "#Grid search over hyperparameter space, randomised. \n",
    "if(random_grid_search):\n",
    "    n_iter_v = int(sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2\n",
    "\n",
    "    grid= RandomizedSearchCV(current_algorithm, parameters,\n",
    "                             verbose=1, cv=[(slice(None), slice(None))],\n",
    "                             n_jobs =grid_n_jobs, n_iter = n_iter_v, error_score=np.nan)\n",
    "else:   \n",
    "    grid = GridSearchCV(\n",
    "        current_algorithm, parameters, verbose=3, cv=[(slice(None), slice(None))], n_jobs =grid_n_jobs,\n",
    "        error_score=np.nan,\n",
    "        \n",
    "    )  # Negate CV in param search for speed\n",
    "\n",
    "\n",
    "\n",
    "pg = ParameterGrid(parameter_space)\n",
    "pg = len(pg)\n",
    "print(pg)\n",
    "if pg > 100000:\n",
    "    print(\"grid too large\", str(pg))\n",
    "    raise Exception(\"grid too large\", str(pg))\n",
    "# print(grid)\n",
    "print(\"Full pg\", pg)\n",
    "grid.fit(X, y)    \n",
    "\n",
    "\n",
    "current_algorithm = grid.best_estimator_\n",
    "\n",
    "current_algorithm.fit(X, y)    \n",
    "\n",
    "scores = cross_validate(\n",
    "    current_algorithm,\n",
    "    X,\n",
    "    y,\n",
    "    scoring=metric_list,\n",
    "    cv=cv,\n",
    "    n_jobs=grid_n_jobs,  # Full CV on final best model #exp -1 was 1\n",
    "    pre_dispatch = 80, #exp,\n",
    "    error_score=np.nan,\n",
    "    verbose = 3\n",
    ")\n",
    "current_algorithm_scores = scores\n",
    "\n",
    "current_algorithm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27588f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6975170",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_algorithm.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726736c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_algorithm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f62267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c68943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KNeighborsTimeSeriesClassifier(n_neighbors=2, distance='dtw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d162dbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9942c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model_class_list)):\n",
    "\n",
    "    idx = i\n",
    "\n",
    "    class_name = model_class_list[idx]\n",
    "\n",
    "    args = (\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    X_test,\n",
    "                    y_test, \n",
    "                    class_name.algorithm_implementation,\n",
    "                    class_name.parameter_space,\n",
    "                    class_name.method_name,\n",
    "                    X,\n",
    "                    y,\n",
    "                    #class_name.parameter_space\n",
    "\n",
    "                )\n",
    "\n",
    "    print(f\"Test args set {class_name.method_name}\")\n",
    "\n",
    "    sub_sample_param_space_pct = 0.00001\n",
    "\n",
    "    parameter_space = class_name.parameter_space\n",
    "\n",
    "    n_iter_v = int(sub_sample_param_space_pct *  len(ParameterGrid(parameter_space))) + 2\n",
    "\n",
    "    n_iter_v\n",
    "\n",
    "    #%pip install --upgrade tensorflow\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    def get_single_permutation(grid):\n",
    "        # Extract the values from the dictionary grid\n",
    "        values = list(grid.values())\n",
    "\n",
    "        # Get the permutation iterator\n",
    "        permutation_iterator = itertools.permutations(values)\n",
    "\n",
    "        # Get the first permutation from the iterator\n",
    "        single_permutation = next(permutation_iterator)\n",
    "\n",
    "        # Convert the permutation tuple back to a dictionary\n",
    "        result = {key: value for key, value in zip(grid.keys(), single_permutation)}\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Example usage:\n",
    "    grid = class_name.parameter_space\n",
    "\n",
    "    single_permutation = get_single_permutation(grid)\n",
    "    print(single_permutation)\n",
    "\n",
    "\n",
    "    arbitrary_permutation = {}\n",
    "    for key, values in parameter_space.items():\n",
    "        arbitrary_permutation[key] = random.choice(values)\n",
    "\n",
    "    print(arbitrary_permutation)\n",
    "\n",
    "    class_name.algorithm_implementation\n",
    "\n",
    "    model = class_name.algorithm_implementation(**arbitrary_permutation)\n",
    "\n",
    "    arbitrary_permutation\n",
    "\n",
    "    model\n",
    "\n",
    "    X_train.shape\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 3: Compute the Area Under the ROC Curve (AUC) using the predicted labels and true labels\n",
    "    auc_score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 2: Predict the labels for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a3872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "\n",
    "# Get a list of all installed packages\n",
    "installed_packages = pkg_resources.working_set\n",
    "\n",
    "# Print package names and their versions\n",
    "for package in installed_packages:\n",
    "    print(f\"{package.key}=={package.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f45cfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrCIFClassifier(n_estimators=300, n_intervals=30, n_jobs=-1, random_state=0,\n",
    "                time_limit_in_minutes=3). Check the list of available parameters with `object.get_params().keys()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb771b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DrCIFClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c71ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
